{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Imports\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dtype = torch.float"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download MNIST using PyTorch\n",
    "\n",
    "dataset_folder = os.path.join('cached_datasets')\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(dataset_folder, train=True, download=True)\n",
    "test_data = torchvision.datasets.MNIST(dataset_folder, train=True, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train = np.array(train_data.data, dtype=np.float)\n",
    "x_train = np.reshape(x_train.shape[0], -1) / 255\n",
    "\n",
    "x_test = np.array(test_data.data, dtype=np.float)\n",
    "x_test = np.reshape(x_test.shape[0], -1) / 255\n",
    "\n",
    "y_train = np.array(train_data.targets, dtype=np.long)\n",
    "y_test = np.array(test_data.targets, dtype=np.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def current_to_firing_time(x, tau=20, threshold=.2, tmax=1.0, epsilon=1e-7):\n",
    "    index = x < threshold\n",
    "    x = np.clip(x, threshold + epsilon, 1e9)\n",
    "    t = tau * np.log(x / (x - threshold))\n",
    "    t[index] = tmax\n",
    "    return t\n",
    "\n",
    "def sparse_data_generator(X, y, batch_size, nb_steps, nb_units, time_step, shuffle=True):\n",
    "    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=np.long)\n",
    "    number_of_batches = len(X)//batch_size\n",
    "    sample_index = np.arange(len(X))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    tau_eff = 20e-3/time_step\n",
    "    firing_times = np.array(current_to_firing_time(X, tau=tau_eff, tmax=nb_steps), dtype=np.long)\n",
    "    unit_numbers = np.arange(nb_units)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for _ in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            c = firing_times[idx]<nb_steps\n",
    "            times, units = firing_times[idx][c], unit_numbers[c]\n",
    "\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index], device=device, dtype=torch.long)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SpikingNetwork:\n",
    "\n",
    "    def __init__(self, unit_count: List[int], weight_scale=1.0, tau_membrane=5e-3, tau_synapse=10e-3,\n",
    "                 time_step_s=1e-3, spike_fn = SurrGradSpike.apply):\n",
    "        self.layer_weights = []\n",
    "        self.unit_count = unit_count\n",
    "        self.weight_scale = weight_scale\n",
    "        self.spike_fn = spike_fn\n",
    "        self.tau_membrane = tau_membrane\n",
    "        self.tau_synapse = tau_synapse\n",
    "        self.time_step = time_step_s\n",
    "        self.alpha = float(np.exp(-self.time_step / self.tau_synapse))\n",
    "        self.beta = float(np.exp(-self.time_step / self.tau_membrane))\n",
    "\n",
    "        for i in range(len(unit_count) - 1):\n",
    "            # creates weights for pair (ci, c(i+1)) for i from {0, 1, 2, ..., len(C) - 2}\n",
    "            weights = torch.empty((unit_count[i], unit_count[i+1]), device=device, dtype=dtype, requires_grad=True)\n",
    "            self.layer_weights.append(weights)\n",
    "\n",
    "        self.randomize_weights()\n",
    "\n",
    "    def randomize_weights(self):\n",
    "        for i in range(len(self.layer_weights)):\n",
    "            torch.nn.init.normal_(self.layer_weights[i], mean=0.0, std=self.weight_scale / np.sqrt(self.unit_count[i]))\n",
    "\n",
    "    def run(self, inputs, batch_size, num_steps):\n",
    "        current_spikes = inputs\n",
    "        for i in range(len(self.layer_weights) - 1):\n",
    "            hi = torch.einsum('abc, cd -> abd', (current_spikes, self.layer_weights[i]))\n",
    "            synapses = torch.zeros((batch_size, self.unit_count[i+1]), device=device, dtype=dtype)\n",
    "            membranes = torch.zeros((batch_size, self.unit_count[i+1]), device=device, dtype=dtype)\n",
    "\n",
    "            # mem_rec = [membranes]\n",
    "            spike_rec = [membranes]\n",
    "\n",
    "            # compute activity\n",
    "            for dt in range(num_steps):\n",
    "                membrane_threshold = membranes - 1.0\n",
    "                out = self.spike_fn(membrane_threshold)\n",
    "                rst = torch.zeros_like(membranes)\n",
    "                c = (membrane_threshold > 0)\n",
    "                rst[c] = torch.ones_like(membrane_threshold)[c]\n",
    "\n",
    "                new_synapses = self.alpha * synapses + hi[:, dt]\n",
    "                # new_membranes = self.beta * membranes + synapses - rst\n",
    "\n",
    "            current_spikes = spike_rec\n",
    "\n",
    "        # readout\n",
    "        h_final = torch.einsum('abc, cd -> abd', (current_spikes, self.layer_weights[-1]))\n",
    "        flt = torch.zeros((batch_size, self.unit_count[-1]), device=device, dtype=dtype)\n",
    "        out = torch.zeros((batch_size, self.unit_count[-1]), device=device, dtype=dtype)\n",
    "        out_rec = [out]\n",
    "        for dt in range(num_steps):\n",
    "            new_flt = self.alpha * flt + h_final[:, dt]\n",
    "            new_out = self.beta * out + flt\n",
    "\n",
    "            flt = new_flt\n",
    "            out = new_out\n",
    "\n",
    "            out_rec.append(out)\n",
    "\n",
    "        return out_rec\n",
    "\n",
    "    def train(self, x_data, y_data, batch_size, num_steps, lr=2e-3, num_epochs=10):\n",
    "        params = [self.layer_weights]\n",
    "        optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "        log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n",
    "        loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "        loss_hist = []\n",
    "        for epoch in range(num_epochs):\n",
    "            local_loss = []\n",
    "\n",
    "            for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, self.unit_count[0]):\n",
    "                output = self.run(x_local.to_dense(), batch_size, num_steps)\n",
    "                m, _ = torch.max(output, 1)\n",
    "                log_p_y = log_softmax_fn(m)\n",
    "                loss_val = loss_fn(log_p_y, y_local)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "                local_loss.append(loss_val.item())\n",
    "\n",
    "            mean_loss = np.mean(local_loss)\n",
    "            loss_hist.append(mean_loss)\n",
    "            print('Epoch {}: loss={.5f}'.format(epoch + 1, mean_loss))\n",
    "\n",
    "        return loss_hist\n",
    "\n",
    "    def eval_acc(self, x, y, batch_size, num_steps):\n",
    "        accs = []\n",
    "        for x_local, y_local in sparse_data_generator(x, y, batch_size, num_steps,\n",
    "                                                      self.unit_count[0], time_step=self.time_step, shuffle=False):\n",
    "            output = self.run(x_local.to_dense(), batch_size, num_steps)\n",
    "            max, _ = torch.max(output, 1)\n",
    "            _, argmax = torch.max(max, 1)\n",
    "            temp = np.mean((y_local == argmax).detach().cpu().numpy())\n",
    "            accs.append(temp)\n",
    "\n",
    "        return np.mean(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}