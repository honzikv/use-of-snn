{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Imports\n"
    }
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dtype = torch.float"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# Download MNIST using PyTorch\n",
    "\n",
    "dataset_folder = os.path.join('cached_datasets')\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(dataset_folder, train=True, transform=None, target_transform=None, download=True)\n",
    "test_data = torchvision.datasets.MNIST(dataset_folder, train=False, transform=None, target_transform=None, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "x_train = np.array(train_data.data, dtype=np.float)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
    "\n",
    "x_test = np.array(test_data.data, dtype=np.float)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
    "\n",
    "y_train = np.array(train_data.targets, dtype=np.long)\n",
    "y_test = np.array(test_data.targets, dtype=np.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def current_to_firing_time(x, tau=20, threshold=.2, tmax=1.0, epsilon=1e-7):\n",
    "    index = x < threshold\n",
    "    x = np.clip(x, threshold + epsilon, 1e9)\n",
    "    t = tau * np.log(x / (x - threshold))\n",
    "    t[index] = tmax\n",
    "    return t\n",
    "\n",
    "def sparse_data_generator(X, y, batch_size, nb_steps, nb_units, time_step, shuffle=True):\n",
    "    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "    labels_ = np.array(y,dtype=np.long)\n",
    "    number_of_batches = len(X)//batch_size\n",
    "    sample_index = np.arange(len(X))\n",
    "    # compute discrete firing times\n",
    "    tau_eff = 20e-3/time_step\n",
    "    firing_times = np.array(current_to_firing_time(X, tau=tau_eff, tmax=nb_steps), dtype=np.long)\n",
    "    unit_numbers = np.arange(nb_units)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for _ in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            c = firing_times[idx]<nb_steps\n",
    "            times, units = firing_times[idx][c], unit_numbers[c]\n",
    "\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index], device=device, dtype=torch.long)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class SpikingNetwork:\n",
    "\n",
    "    def __init__(self, unit_count: List[int], weight_scale=1.0, tau_membrane=5e-3, tau_synapse=10e-3,\n",
    "                 time_step_s=1e-3, spike_fn = SurrGradSpike.apply):\n",
    "        self.layer_weights = []\n",
    "        self.unit_count = unit_count\n",
    "        self.weight_scale = weight_scale\n",
    "        self.spike_fn = spike_fn\n",
    "        self.tau_membrane = tau_membrane\n",
    "        self.tau_synapse = tau_synapse\n",
    "        self.time_step = time_step_s\n",
    "        self.alpha = float(np.exp(-self.time_step / self.tau_synapse))\n",
    "        self.beta = float(np.exp(-self.time_step / self.tau_membrane))\n",
    "\n",
    "        for i in range(len(unit_count) - 1):\n",
    "            # creates weights for pair (ci, c(i+1)) for i from {0, 1, 2, ..., len(C) - 2}\n",
    "            weights = torch.empty((unit_count[i], unit_count[i+1]), device=device, dtype=dtype, requires_grad=True)\n",
    "            self.layer_weights.append(weights)\n",
    "\n",
    "        self.randomize_weights()\n",
    "\n",
    "    def randomize_weights(self):\n",
    "        for i in range(len(self.layer_weights)):\n",
    "            torch.nn.init.normal_(self.layer_weights[i], mean=0.0, std=self.weight_scale / np.sqrt(self.unit_count[i]))\n",
    "\n",
    "    def run(self, inputs, batch_size, num_steps):\n",
    "        current_spikes = inputs\n",
    "        for i in range(len(self.layer_weights) - 1):\n",
    "            hi = torch.einsum('abc, cd -> abd', (current_spikes, self.layer_weights[i]))\n",
    "            syn = torch.zeros((batch_size, self.unit_count[i+1]), device=device, dtype=dtype)\n",
    "            mem = torch.zeros((batch_size, self.unit_count[i+1]), device=device, dtype=dtype)\n",
    "\n",
    "            mem_rec = [mem]\n",
    "            spike_rec = [mem]\n",
    "\n",
    "            # compute activity\n",
    "            for dt in range(num_steps):\n",
    "                mthr = mem - 1.0\n",
    "                out = spike_fn(mthr)\n",
    "                rst = torch.zeros_like(mem)\n",
    "                c = (mthr > 0)\n",
    "                rst[c] = torch.ones_like(mem)[c]\n",
    "\n",
    "                new_syn = self.alpha * syn + hi[:, dt]\n",
    "                new_mem = self.beta * mem + syn - rst\n",
    "\n",
    "                mem = new_mem\n",
    "                syn = new_syn\n",
    "\n",
    "                mem_rec.append(mem)\n",
    "                spike_rec.append(out)\n",
    "\n",
    "            current_spikes = spike_rec\n",
    "            current_spikes = torch.stack(mem_rec, dim=1)\n",
    "\n",
    "        # readout\n",
    "\n",
    "\n",
    "        h_final = torch.einsum('abc, cd -> abd', (current_spikes, self.layer_weights[-1]))\n",
    "        flt = torch.zeros((batch_size, self.unit_count[-1]), device=device, dtype=dtype)\n",
    "        out = torch.zeros((batch_size, self.unit_count[-1]), device=device, dtype=dtype)\n",
    "        out_rec = [out]\n",
    "        for dt in range(num_steps):\n",
    "            new_flt = self.alpha * flt + h_final[:, dt]\n",
    "            new_out = self.beta * out + flt\n",
    "\n",
    "            flt = new_flt\n",
    "            out = new_out\n",
    "\n",
    "            out_rec.append(out)\n",
    "\n",
    "        out_rec = torch.stack(out_rec, dim=1)\n",
    "        return out_rec\n",
    "\n",
    "    def train(self, x_data, y_data, batch_size, num_steps, lr=2e-3, num_epochs=10):\n",
    "        params = self.layer_weights\n",
    "        optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "        log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n",
    "        loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "        loss_hist = []\n",
    "        for epoch in range(num_epochs):\n",
    "            local_loss = []\n",
    "\n",
    "            for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, self.unit_count[0],\n",
    "                                                          self.time_step):\n",
    "                output = self.run(x_local.to_dense(), batch_size, num_steps)\n",
    "                m, _ = torch.max(output, 1)\n",
    "                log_p_y = log_softmax_fn(m)\n",
    "                loss_val = loss_fn(log_p_y, y_local)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "                local_loss.append(loss_val.item())\n",
    "\n",
    "            mean_loss = np.mean(local_loss)\n",
    "            loss_hist.append(mean_loss)\n",
    "            print('Epoch {}: loss={:.5f}'.format(epoch + 1, mean_loss))\n",
    "\n",
    "        return loss_hist\n",
    "\n",
    "    def eval_acc(self, x, y, batch_size, num_steps):\n",
    "        accs = []\n",
    "        for x_local, y_local in sparse_data_generator(x, y, batch_size, num_steps,\n",
    "                                                      self.unit_count[0], time_step=self.time_step, shuffle=False):\n",
    "            output = self.run(x_local.to_dense(), batch_size, num_steps)\n",
    "            m, _ = torch.max(output, 1)\n",
    "            _, argmax = torch.max(m, 1)\n",
    "            temp = np.mean((y_local == argmax).detach().cpu().numpy())\n",
    "            accs.append(temp)\n",
    "\n",
    "        return np.mean(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=64.27571\n",
      "Epoch 2: loss=2.53018\n",
      "Epoch 3: loss=2.35099\n",
      "Epoch 4: loss=2.27043\n",
      "Epoch 5: loss=2.22924\n",
      "Epoch 6: loss=2.20289\n",
      "Epoch 7: loss=2.18941\n",
      "Epoch 8: loss=2.17966\n",
      "Epoch 9: loss=2.16984\n",
      "Epoch 10: loss=2.16668\n",
      "Epoch 11: loss=2.15855\n",
      "Epoch 12: loss=2.15943\n",
      "Epoch 13: loss=2.14415\n",
      "Epoch 14: loss=2.14247\n",
      "Epoch 15: loss=2.12386\n",
      "Epoch 16: loss=2.12615\n",
      "Epoch 17: loss=2.11801\n",
      "Epoch 18: loss=2.10779\n",
      "Epoch 19: loss=2.11416\n",
      "Epoch 20: loss=2.10682\n",
      "Epoch 21: loss=2.09620\n",
      "Epoch 22: loss=2.09414\n",
      "Epoch 23: loss=2.09377\n",
      "Epoch 24: loss=2.09423\n",
      "Epoch 25: loss=2.08478\n",
      "Epoch 26: loss=2.08330\n",
      "Epoch 27: loss=2.07974\n"
     ]
    }
   ],
   "source": [
    "neurons = [28*28, 100, 100, 10]\n",
    "tau_mem, tau_syn = 10e-3, 5e-3\n",
    "batch_size = 256\n",
    "time_step_dur,num_steps = 1e-3, 100\n",
    "\n",
    "weight_scale = 10 * (1.0 - float(np.exp(-time_step_dur/tau_mem)))\n",
    "steps = 100\n",
    "model = SpikingNetwork(neurons, weight_scale=weight_scale, tau_membrane=tau_mem,\n",
    "                       tau_synapse=tau_syn, time_step_s=time_step_dur)\n",
    "\n",
    "hist = model.train(x_train, y_train, batch_size, num_steps=num_steps, num_epochs=30, lr=2e-4)\n",
    "\n",
    "plt.figure(figsize=(3.3,2),dpi=150)\n",
    "plt.plot(hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_acc = model.eval_acc(x_train, y_train, batch_size, num_steps)\n",
    "test_acc = model.eval_acc(x_test, y_test, batch_size, num_steps)\n",
    "\n",
    "print('Training accuracy: {:.5f}'.format(train_acc))\n",
    "print('Testing accuracy: {:.5f}'.format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9c9070f4",
   "language": "python",
   "display_name": "PyCharm (use-of-snn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}