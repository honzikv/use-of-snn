{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(0) # seed for consistency\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def convert_to_firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\n",
    "    idx = x < thr\n",
    "    x = np.clip(x, thr + epsilon, 1e9)\n",
    "    T = tau * np.log(x / (x - thr))\n",
    "    T[idx] = tmax\n",
    "    return T\n",
    "\n",
    "def sparse_data_generator(x, y, batch_size, num_steps, num_units, time_step=1e-3, shuffle=True):\n",
    "    labels_ = np.array(y, dtype=np.int)\n",
    "    number_of_batches = len(x) // batch_size\n",
    "    sample_index = np.arange(len(x))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    tau_eff = 20e-3 / time_step\n",
    "    firing_times = np.array(convert_to_firing_time(x, tau=tau_eff, tmax=num_steps), dtype=np.int)\n",
    "    unit_numbers = np.arange(num_units)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    counter = 0\n",
    "    while counter < number_of_batches:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "\n",
    "        coo = [[] for _ in range(3)]\n",
    "        for bc, idx in enumerate(batch_index):\n",
    "            c = firing_times[idx] < num_steps\n",
    "            times, units = firing_times[idx][c], unit_numbers[c]\n",
    "\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size, num_steps, num_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index], device=device, dtype=torch.long)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    scale = 100.0  # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input / (SurrGradSpike.scale * torch.abs(input) + 1.0) ** 2\n",
    "        return grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "class DeepSNNModel:\n",
    "    \"\"\"\n",
    "    This class implements a simple deep snn model that comprises a set of fully connected layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units: List[int], weight_scale: Tuple = (7.0, 1.0), recurrent=False, time_step=1e-3,\n",
    "                 tau_mem=10e-3,\n",
    "                 tau_syn=5e-3):\n",
    "        \"\"\"\n",
    "        :param units: list of units\n",
    "        :param weight_scale: tuple of one or two values, controls the scaling of the weights\n",
    "        :param recurrent: whether the network is recurrent one - additional recurrent weights are used\n",
    "        :param time_step: duration of timestep in seconds\n",
    "        :param tau_mem: membrane tau parameter\n",
    "        :param tau_syn: synapse tau parameter\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.tau_mem = tau_mem\n",
    "        self.tau_syn = tau_syn\n",
    "        self.alpha = float(np.exp(-time_step / tau_syn))\n",
    "        self.beta = float(np.exp(-time_step / tau_mem))\n",
    "        self.is_recurrent = recurrent\n",
    "\n",
    "        if len(weight_scale) == 2:\n",
    "            self.weight_scale = weight_scale[0] * (weight_scale[1] - self.beta)\n",
    "        else:\n",
    "            self.weight_scale = weight_scale[0]\n",
    "        self.weights = self.init_weights()\n",
    "        self.recurrent_weights = None if not recurrent else self.init_recurrent_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes weights between layers\n",
    "        :return: list of torch tensors representing weights\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        for i in range(len(self.units) - 1):\n",
    "            wi = torch.empty((self.units[i], self.units[i + 1]), device=device, dtype=dtype, requires_grad=True)\n",
    "            torch.nn.init.normal_(wi, mean=0.0, std=self.weight_scale / np.sqrt(self.units[i]))\n",
    "            weights.append(wi)\n",
    "        return weights\n",
    "\n",
    "    def init_recurrent_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes recurrent weights for recurrent network for every hidden layer\n",
    "        :return: list of torch tensors representing recurrent weights\n",
    "        \"\"\"\n",
    "        recurrent_weights = []\n",
    "        for i in range(1, len(self.units) - 1):\n",
    "            vi = torch.empty((self.units[i], self.units[i]), device=device, dtype=dtype, requires_grad=True)\n",
    "            torch.nn.init.normal_(vi, mean=0.0, std=self.weight_scale / np.sqrt(self.units[i]))\n",
    "            recurrent_weights.append(vi)\n",
    "        return recurrent_weights\n",
    "\n",
    "    def run_recurrent(self, inputs, batch_size, steps, spike_fn=SurrGradSpike.apply):\n",
    "        \"\"\"\n",
    "        Runs recurrent network\n",
    "        :param inputs: input data\n",
    "        :param batch_size: batch size\n",
    "        :param steps: number of timesteps\n",
    "        :param spike_fn: surrogate gradient function\n",
    "        :return: output layer results and\n",
    "        \"\"\"\n",
    "        layer_outputs = [inputs]\n",
    "        mem_recs = []\n",
    "        spike_recs = []\n",
    "\n",
    "        # compute activity in each hidden layer\n",
    "        for i in range(1, len(self.units) - 1):  # current hidden layer index\n",
    "            hidden_units = self.units[i]  # neurons in current hidden layer\n",
    "            syn_hidden_i = torch.zeros((batch_size, hidden_units), device=device, dtype=dtype)\n",
    "            mem_hidden_i = torch.zeros((batch_size, hidden_units), device=device, dtype=dtype)\n",
    "\n",
    "            mem_rec_hidden_i = [mem_hidden_i]\n",
    "            spike_rec_hidden_i = [mem_hidden_i]\n",
    "\n",
    "            hi = torch.zeros((batch_size, hidden_units), device=device, dtype=dtype)\n",
    "            hi_from_prev_layer = torch.einsum('abc, cd -> abd', (layer_outputs[i - 1], self.weights[i - 1]))\n",
    "\n",
    "            for dt in range(steps):\n",
    "                mem_threshold = mem_hidden_i - 1.0\n",
    "                spike_out = spike_fn(mem_threshold)\n",
    "                rst = torch.zeros_like(mem_hidden_i)\n",
    "                c = (mem_threshold > 0)\n",
    "                rst[c] = torch.ones_like(mem_hidden_i)[c]\n",
    "\n",
    "                hi = hi_from_prev_layer[:, dt] + torch.einsum('ab, bc -> ac', (hi, self.recurrent_weights[i - 1]))\n",
    "                new_syn = self.alpha * syn_hidden_i + hi\n",
    "                new_mem = self.beta * mem_hidden_i + syn_hidden_i - rst\n",
    "\n",
    "                mem_hidden_i = new_mem\n",
    "                syn_hidden_i = new_syn\n",
    "\n",
    "                mem_rec_hidden_i.append(mem_hidden_i)\n",
    "                spike_rec_hidden_i.append(spike_out)\n",
    "\n",
    "            spike_rec_hidden_i = torch.stack(spike_rec_hidden_i, dim=1)\n",
    "            mem_rec_hidden_i = torch.stack(mem_rec_hidden_i, dim=1)\n",
    "\n",
    "            layer_outputs.append(spike_rec_hidden_i)  # append output so it can be fed to the next hidden layer\n",
    "            mem_recs.append(mem_rec_hidden_i)  # append records\n",
    "            spike_recs.append(spike_rec_hidden_i)  # append records\n",
    "\n",
    "            # readout layer\n",
    "        hn = torch.einsum('abc, cd -> abd', (layer_outputs[-1], self.weights[-1]))\n",
    "        flt = torch.zeros((batch_size, self.units[-1]), device=device, dtype=dtype)\n",
    "        spike_out = torch.zeros((batch_size, self.units[-1]), device=device, dtype=dtype)\n",
    "\n",
    "        out_rec = [spike_out]\n",
    "        for dt in range(steps):\n",
    "            new_flt = self.alpha * flt + hn[:, dt]\n",
    "            new_out = self.beta * spike_out + flt\n",
    "\n",
    "            flt = new_flt\n",
    "            spike_out = new_out\n",
    "\n",
    "            out_rec.append(spike_out)\n",
    "\n",
    "        out_rec = torch.stack(out_rec, dim=1)\n",
    "        return out_rec, spike_recs, layer_outputs, mem_recs\n",
    "\n",
    "    def run_feed_forward(self, inputs, batch_size, steps, spike_fn=SurrGradSpike.apply):\n",
    "        layer_outputs = [inputs]\n",
    "        mem_recs = []\n",
    "        spike_recs = []\n",
    "\n",
    "        # compute activity of every hidden layer\n",
    "        # hidden layers are stored from index 1 to index len(self.units) - 2\n",
    "        for i in range(1, len(self.units) - 1):\n",
    "            hidden_units = self.units[i]  # units in next layer\n",
    "\n",
    "            # sum of output from previous layer and weights of current hidden layer\n",
    "            hi = torch.einsum('abc,cd->abd', (layer_outputs[i - 1], self.weights[i - 1]))\n",
    "            syn_hidden_i = torch.zeros((batch_size, hidden_units), device=device, dtype=dtype)  # synapses\n",
    "            mem_hidden_i = torch.zeros((batch_size, hidden_units), device=device, dtype=dtype)  # membranes\n",
    "\n",
    "            mem_rec_hidden_i = [mem_hidden_i]\n",
    "            spike_rec_hidden_i = [mem_hidden_i]\n",
    "\n",
    "            for dt in range(steps):\n",
    "                mem_threshold = mem_hidden_i - 1.0\n",
    "                spike_out = spike_fn(mem_threshold)\n",
    "                rst = torch.zeros_like(mem_hidden_i)\n",
    "                c = (mem_threshold > 0)\n",
    "                rst[c] = torch.ones_like(mem_hidden_i)[c]\n",
    "\n",
    "                new_syn = self.alpha * syn_hidden_i + hi[:, dt]\n",
    "                new_mem = self.beta * mem_hidden_i + syn_hidden_i - rst\n",
    "                mem_hidden_i = new_mem\n",
    "                syn_hidden_i = new_syn\n",
    "\n",
    "                mem_rec_hidden_i.append(mem_hidden_i)\n",
    "                spike_rec_hidden_i.append(spike_out)\n",
    "\n",
    "            spike_rec_hidden_i = torch.stack(spike_rec_hidden_i, dim=1)\n",
    "            mem_rec_hidden_i = torch.stack(mem_rec_hidden_i, dim=1)\n",
    "\n",
    "            layer_outputs.append(spike_rec_hidden_i)  # append output so it can be fed to the next hidden layer\n",
    "            mem_recs.append(mem_rec_hidden_i)  # append records\n",
    "            spike_recs.append(spike_rec_hidden_i)  # append records\n",
    "\n",
    "        # readout layer\n",
    "        hn = torch.einsum('abc,cd->abd', (layer_outputs[-1], self.weights[-1]))\n",
    "        flt = torch.zeros((batch_size, self.units[-1]), device=device, dtype=dtype)\n",
    "        spike_out = torch.zeros((batch_size, self.units[-1]), device=device, dtype=dtype)\n",
    "\n",
    "        out_rec = [spike_out]\n",
    "        for dt in range(steps):\n",
    "            new_flt = self.alpha * flt + hn[:, dt]\n",
    "            new_out = self.beta * spike_out + flt\n",
    "            flt = new_flt\n",
    "            spike_out = new_out\n",
    "            out_rec.append(spike_out)\n",
    "\n",
    "        out_rec = torch.stack(out_rec, dim=1)\n",
    "        return out_rec, spike_recs, layer_outputs, mem_recs\n",
    "\n",
    "    def train(self, x_data, y_data, batch_size, num_steps=100, time_step=1e-3, lr=1e-3, num_epochs=10,\n",
    "              use_regularizer=False):\n",
    "\n",
    "        if not use_regularizer:\n",
    "            optimizer = torch.optim.Adam(self.weights, lr=lr, betas=(0.9, 0.999))\n",
    "        else:\n",
    "            optimizer = torch.optim.Adamax(self.weights, lr=lr, betas=(0.9, 0.999))\n",
    "        log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "        loss_fn = nn.NLLLoss()\n",
    "\n",
    "        loss_hist = []\n",
    "        for epoch in range(num_epochs):\n",
    "            local_loss = []\n",
    "            for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, self.units[0],\n",
    "                                                          time_step):\n",
    "                # todo simplify\n",
    "                if not self.is_recurrent:  # if the network does not contain recurrent weights run it as feedforward\n",
    "                    output, spike_recs, layer_outputs, mem_recs = self.run_feed_forward(x_local.to_dense(), batch_size,\n",
    "                                                                                        num_steps)\n",
    "                else:\n",
    "                    output, spike_recs, layer_outputs, mem_recs = self.run_recurrent(x_local.to_dense(), batch_size,\n",
    "                                                                                     num_steps)\n",
    "\n",
    "                output_max, _ = torch.max(output, 1)\n",
    "                log_p_y = log_softmax_fn(output_max)\n",
    "\n",
    "                loss_val = loss_fn(log_p_y, y_local)\n",
    "                if use_regularizer:\n",
    "                    pass\n",
    "                    # reg_loss = 0\n",
    "                    # for spike_rec in spike_recs:\n",
    "                    #     reg_loss += torch.sum(spike_rec)\n",
    "                    # reg_loss *= 1e-5\n",
    "                    #\n",
    "                    # l2_reg_loss = 0\n",
    "                    # for spike_rec in spike_recs:\n",
    "                    #     l2_reg_loss += torch.mean(torch.sum(torch.sum(spike_rec, dim=0), dim=0) ** 2)\n",
    "                    # l2_reg_loss *= 1e-5\n",
    "                    # reg_loss += l2_reg_loss\n",
    "                    # loss_val += reg_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "                local_loss.append(loss_val.item())\n",
    "\n",
    "            mean_loss = np.mean(local_loss)\n",
    "            print(\"Epoch %i: loss=%.5f\" % (epoch + 1, mean_loss))\n",
    "            loss_hist.append(mean_loss)\n",
    "\n",
    "        return loss_hist\n",
    "\n",
    "\n",
    "def compute_classification_accuracy(x_data, y_data, batch_size, snn_model: DeepSNNModel, time_step=1e-3, num_steps=100):\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, snn_model.units[0], time_step,\n",
    "                                                  False):\n",
    "        if not snn_model.recurrent_weights:\n",
    "            output, spike_recs, layer_outputs, mem_recs = snn_model.run_feed_forward(x_local.to_dense(), batch_size,\n",
    "                                                                                     num_steps)\n",
    "        else:\n",
    "            output, spike_recs, layer_outputs, mem_recs = snn_model.run_recurrent(x_local.to_dense(), batch_size,\n",
    "                                                                                  num_steps)\n",
    "\n",
    "        output_max, _ = torch.max(output, 1)\n",
    "        _, output_argmax = torch.max(output_max, 1)\n",
    "        tmp = np.mean((y_local == output_argmax).detach().cpu().numpy())\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "dataset_folder = os.path.join('cached_datasets')\n",
    "mnist_acc, fashion_mnist_acc = [], []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark on MNIST dataset\n",
      "Epoch 1: loss=0.40347\n",
      "Epoch 2: loss=0.18033\n",
      "Epoch 3: loss=0.13489\n",
      "Epoch 4: loss=0.10992\n",
      "Epoch 5: loss=0.10074\n",
      "Epoch 6: loss=0.08079\n",
      "Epoch 7: loss=0.07507\n",
      "Epoch 8: loss=0.07118\n",
      "Epoch 9: loss=0.06344\n",
      "Epoch 10: loss=0.06011\n",
      "Epoch 11: loss=0.05657\n",
      "Epoch 12: loss=0.05430\n",
      "Epoch 13: loss=0.05006\n",
      "Epoch 14: loss=0.04669\n",
      "Epoch 15: loss=0.04478\n",
      "Epoch 16: loss=0.04418\n",
      "Epoch 17: loss=0.04819\n",
      "Epoch 18: loss=0.04554\n",
      "Epoch 19: loss=0.03430\n",
      "Epoch 20: loss=0.03410\n",
      "Epoch 21: loss=0.03461\n",
      "Epoch 22: loss=0.03563\n",
      "Epoch 23: loss=0.03673\n",
      "Epoch 24: loss=0.03384\n",
      "Epoch 25: loss=0.03179\n",
      "Epoch 26: loss=0.02700\n",
      "Epoch 27: loss=0.02183\n",
      "Epoch 28: loss=0.02849\n",
      "Epoch 29: loss=0.02779\n",
      "Epoch 30: loss=0.03555\n",
      "Model 1, accuracy: 0.96985\n",
      "Epoch 1: loss=0.74547\n",
      "Epoch 2: loss=0.27163\n",
      "Epoch 3: loss=0.21572\n",
      "Epoch 4: loss=0.18414\n",
      "Epoch 5: loss=0.15780\n",
      "Epoch 6: loss=0.14357\n",
      "Epoch 7: loss=0.13405\n",
      "Epoch 8: loss=0.12692\n",
      "Epoch 9: loss=0.12354\n",
      "Epoch 10: loss=0.12305\n",
      "Epoch 11: loss=0.10691\n",
      "Epoch 12: loss=0.10474\n",
      "Epoch 13: loss=0.09290\n",
      "Epoch 14: loss=0.09832\n",
      "Epoch 15: loss=0.09093\n",
      "Epoch 16: loss=0.08102\n",
      "Epoch 17: loss=0.08183\n",
      "Epoch 18: loss=0.08637\n",
      "Epoch 19: loss=0.07992\n",
      "Epoch 20: loss=0.07783\n",
      "Epoch 21: loss=0.08015\n",
      "Epoch 22: loss=0.07417\n",
      "Epoch 23: loss=0.07047\n",
      "Epoch 24: loss=0.06901\n",
      "Epoch 25: loss=0.06879\n",
      "Epoch 26: loss=0.08083\n",
      "Epoch 27: loss=0.06850\n",
      "Epoch 28: loss=0.06287\n",
      "Epoch 29: loss=0.06080\n",
      "Epoch 30: loss=0.06449\n",
      "Model 2, accuracy: 0.96605\n",
      "Epoch 1: loss=0.51458\n",
      "Epoch 2: loss=0.28397\n",
      "Epoch 3: loss=0.20630\n",
      "Epoch 4: loss=0.18360\n",
      "Epoch 5: loss=0.17303\n",
      "Epoch 6: loss=0.15864\n",
      "Epoch 7: loss=0.14605\n",
      "Epoch 8: loss=0.13620\n",
      "Epoch 9: loss=0.14171\n",
      "Epoch 10: loss=0.13248\n",
      "Epoch 11: loss=0.11309\n",
      "Epoch 12: loss=0.12037\n",
      "Epoch 13: loss=0.12726\n",
      "Epoch 14: loss=0.12662\n",
      "Epoch 15: loss=0.11709\n",
      "Epoch 16: loss=0.10921\n",
      "Epoch 17: loss=0.11158\n",
      "Epoch 18: loss=0.12649\n",
      "Epoch 19: loss=0.11035\n",
      "Epoch 20: loss=0.11275\n",
      "Epoch 21: loss=0.12099\n",
      "Epoch 22: loss=0.10615\n",
      "Epoch 23: loss=0.10462\n",
      "Epoch 24: loss=0.10381\n",
      "Epoch 25: loss=0.11086\n",
      "Epoch 26: loss=0.10271\n",
      "Epoch 27: loss=0.10760\n",
      "Epoch 28: loss=0.11273\n",
      "Epoch 29: loss=0.10553\n",
      "Epoch 30: loss=0.09725\n",
      "Model 3, accuracy: 0.95883\n",
      "Epoch 1: loss=1.13548\n",
      "Epoch 2: loss=0.50936\n",
      "Epoch 3: loss=0.36254\n",
      "Epoch 4: loss=0.33731\n",
      "Epoch 5: loss=0.29280\n",
      "Epoch 6: loss=0.28276\n",
      "Epoch 7: loss=0.23062\n",
      "Epoch 8: loss=0.21895\n",
      "Epoch 9: loss=0.21725\n",
      "Epoch 10: loss=0.20397\n",
      "Epoch 11: loss=0.19890\n",
      "Epoch 12: loss=0.19017\n",
      "Epoch 13: loss=0.22878\n",
      "Epoch 14: loss=0.19256\n",
      "Epoch 15: loss=0.17575\n",
      "Epoch 16: loss=0.17058\n",
      "Epoch 17: loss=0.17568\n",
      "Epoch 18: loss=0.17298\n",
      "Epoch 19: loss=0.17309\n",
      "Epoch 20: loss=0.18364\n",
      "Epoch 21: loss=0.16695\n",
      "Epoch 22: loss=0.16785\n",
      "Epoch 23: loss=0.17869\n",
      "Epoch 24: loss=0.17546\n",
      "Epoch 25: loss=0.17256\n",
      "Epoch 26: loss=0.18171\n",
      "Epoch 27: loss=0.20098\n",
      "Epoch 28: loss=0.20272\n",
      "Epoch 29: loss=0.20879\n",
      "Epoch 30: loss=0.20340\n",
      "Model 4, accuracy: 0.93830\n"
     ]
    }
   ],
   "source": [
    "print('Running benchmark on MNIST dataset')\n",
    "train_dataset = torchvision.datasets.MNIST(dataset_folder, train=True,\n",
    "                                           transform=None, target_transform=None, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(dataset_folder, train=False,\n",
    "                                          transform=None, target_transform=None, download=True)\n",
    "\n",
    "models_mnist = [\n",
    "    DeepSNNModel([28*28, 256, 128, 10]),\n",
    "    DeepSNNModel([28*28, 256, 128, 64, 10]),\n",
    "    DeepSNNModel([28*28, 256, 128, 10], recurrent=True),\n",
    "    DeepSNNModel([28*28, 256, 128, 64, 10], recurrent=True),\n",
    "]\n",
    "\n",
    "# MNIST standardization\n",
    "mnist_x_train = np.array(train_dataset.data, dtype=np.float)\n",
    "mnist_x_train = mnist_x_train.reshape(mnist_x_train.shape[0], -1) / 255\n",
    "mnist_x_test = np.array(test_dataset.data, dtype=np.float)\n",
    "mnist_x_test = mnist_x_test.reshape(mnist_x_test.shape[0], -1) / 255\n",
    "\n",
    "mnist_y_train = np.array(train_dataset.targets, dtype=np.int)\n",
    "mnist_y_test = np.array(test_dataset.targets, dtype=np.int)\n",
    "\n",
    "i = 1\n",
    "for model in models_mnist:\n",
    "    model.train(mnist_x_train, mnist_y_train, 256, num_epochs=30)\n",
    "    acc = compute_classification_accuracy(mnist_x_test, mnist_y_test, 256, model)\n",
    "    print('Model {}, accuracy: {:.5f}'.format(i, acc))\n",
    "    mnist_acc.append(acc)\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.92969\n",
      "Epoch 2: loss=0.55797\n",
      "Epoch 3: loss=0.47387\n",
      "Epoch 4: loss=0.43576\n",
      "Epoch 5: loss=0.41582\n",
      "Epoch 6: loss=0.41093\n",
      "Epoch 7: loss=0.39499\n",
      "Epoch 8: loss=0.38457\n",
      "Epoch 9: loss=0.37689\n",
      "Epoch 10: loss=0.36526\n",
      "Epoch 11: loss=0.35454\n",
      "Epoch 12: loss=0.34739\n",
      "Epoch 13: loss=0.34859\n",
      "Epoch 14: loss=0.33944\n",
      "Epoch 15: loss=0.33555\n",
      "Epoch 16: loss=0.32322\n",
      "Epoch 17: loss=0.32340\n",
      "Epoch 18: loss=0.31768\n",
      "Epoch 19: loss=0.31241\n",
      "Epoch 20: loss=0.31459\n",
      "Epoch 21: loss=0.31249\n",
      "Epoch 22: loss=0.31338\n",
      "Epoch 23: loss=0.30490\n",
      "Epoch 24: loss=0.29913\n",
      "Epoch 25: loss=0.29269\n",
      "Epoch 26: loss=0.30508\n",
      "Epoch 27: loss=0.30652\n",
      "Epoch 28: loss=0.29266\n",
      "Epoch 29: loss=0.29965\n",
      "Epoch 30: loss=0.28061\n",
      "Model 1, accuracy: 0.86078\n",
      "Epoch 1: loss=1.57793\n",
      "Epoch 2: loss=0.83117\n",
      "Epoch 3: loss=0.53272\n",
      "Epoch 4: loss=0.49433\n",
      "Epoch 5: loss=0.47878\n",
      "Epoch 6: loss=0.46234\n",
      "Epoch 7: loss=0.44445\n",
      "Epoch 8: loss=0.42809\n",
      "Epoch 9: loss=0.43004\n",
      "Epoch 10: loss=0.42607\n",
      "Epoch 11: loss=0.41438\n",
      "Epoch 12: loss=0.40295\n",
      "Epoch 13: loss=0.40429\n",
      "Epoch 14: loss=0.40662\n",
      "Epoch 15: loss=0.40343\n",
      "Epoch 16: loss=0.38395\n",
      "Epoch 17: loss=0.38121\n",
      "Epoch 18: loss=0.39586\n",
      "Epoch 19: loss=0.40166\n",
      "Epoch 20: loss=0.39126\n",
      "Epoch 21: loss=0.38449\n",
      "Epoch 22: loss=0.37023\n",
      "Epoch 23: loss=0.38357\n",
      "Epoch 24: loss=0.39282\n",
      "Epoch 25: loss=0.36885\n",
      "Epoch 26: loss=0.37962\n",
      "Epoch 27: loss=0.37169\n",
      "Epoch 28: loss=0.36205\n",
      "Epoch 29: loss=0.36910\n",
      "Epoch 30: loss=0.36519\n",
      "Model 2, accuracy: 0.84635\n",
      "Epoch 1: loss=1.68388\n",
      "Epoch 2: loss=1.26333\n",
      "Epoch 3: loss=1.17996\n",
      "Epoch 4: loss=0.90601\n",
      "Epoch 5: loss=0.63327\n",
      "Epoch 6: loss=0.58016\n",
      "Epoch 7: loss=0.53985\n",
      "Epoch 8: loss=0.51043\n",
      "Epoch 9: loss=0.47713\n",
      "Epoch 10: loss=0.47467\n",
      "Epoch 11: loss=0.44607\n",
      "Epoch 12: loss=0.43947\n",
      "Epoch 13: loss=0.43269\n",
      "Epoch 14: loss=0.42736\n",
      "Epoch 15: loss=0.41953\n",
      "Epoch 16: loss=0.41448\n",
      "Epoch 17: loss=0.40578\n",
      "Epoch 18: loss=0.40143\n",
      "Epoch 19: loss=0.40499\n",
      "Epoch 20: loss=0.38766\n",
      "Epoch 21: loss=0.38542\n",
      "Epoch 22: loss=0.38404\n",
      "Epoch 23: loss=0.36950\n",
      "Epoch 24: loss=0.38502\n",
      "Epoch 25: loss=0.36887\n",
      "Epoch 26: loss=0.37326\n",
      "Epoch 27: loss=0.37087\n",
      "Epoch 28: loss=0.36370\n",
      "Epoch 29: loss=0.36864\n",
      "Epoch 30: loss=0.39352\n",
      "Model 3, accuracy: 0.80929\n",
      "Epoch 1: loss=1.85568\n",
      "Epoch 2: loss=1.08235\n",
      "Epoch 3: loss=0.90405\n",
      "Epoch 4: loss=0.80624\n",
      "Epoch 5: loss=0.79858\n",
      "Epoch 6: loss=0.78962\n",
      "Epoch 7: loss=0.78901\n",
      "Epoch 8: loss=0.80122\n",
      "Epoch 9: loss=0.77443\n",
      "Epoch 10: loss=0.84846\n",
      "Epoch 11: loss=0.78881\n",
      "Epoch 12: loss=0.80098\n",
      "Epoch 13: loss=0.78216\n",
      "Epoch 14: loss=0.79123\n",
      "Epoch 15: loss=0.76397\n",
      "Epoch 16: loss=0.70147\n",
      "Epoch 17: loss=0.74243\n",
      "Epoch 18: loss=0.73036\n",
      "Epoch 19: loss=0.75287\n",
      "Epoch 20: loss=0.80149\n",
      "Epoch 21: loss=0.78952\n",
      "Epoch 22: loss=0.83583\n",
      "Epoch 23: loss=0.76245\n",
      "Epoch 24: loss=0.78097\n",
      "Epoch 25: loss=0.87556\n",
      "Epoch 26: loss=0.85582\n",
      "Epoch 27: loss=0.84490\n",
      "Epoch 28: loss=0.79332\n",
      "Epoch 29: loss=0.83105\n",
      "Epoch 30: loss=0.86509\n",
      "Model 4, accuracy: 0.69291\n"
     ]
    }
   ],
   "source": [
    "# Fashion MNIST\n",
    "train_dataset = torchvision.datasets.FashionMNIST(dataset_folder, train=True,\n",
    "                                           transform=None, target_transform=None, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(dataset_folder, train=False,\n",
    "                                          transform=None, target_transform=None, download=True)\n",
    "\n",
    "models_fashion_mnist = [\n",
    "    DeepSNNModel([28*28, 256, 128, 10]),\n",
    "    DeepSNNModel([28*28, 256, 128, 64, 10]),\n",
    "    DeepSNNModel([28*28, 256, 128, 10], recurrent=True),\n",
    "    DeepSNNModel([28*28, 256, 128, 64, 10], recurrent=True),\n",
    "]\n",
    "# Standardize the data\n",
    "fmnist_x_train = np.array(train_dataset.data, dtype=np.float)\n",
    "fmnist_x_train = fmnist_x_train.reshape(fmnist_x_train.shape[0], -1) / 255\n",
    "fmnist_x_test = np.array(test_dataset.data, dtype=np.float)\n",
    "fmnist_x_test = fmnist_x_test.reshape(fmnist_x_test.shape[0], -1) / 255\n",
    "\n",
    "fmnist_y_train = np.array(train_dataset.targets, dtype=np.int)\n",
    "fmnist_y_test = np.array(test_dataset.targets, dtype=np.int)\n",
    "\n",
    "i = 1\n",
    "for model in models_fashion_mnist:\n",
    "    model.train(fmnist_x_train, fmnist_y_train, 256, num_epochs=30)\n",
    "    acc = compute_classification_accuracy(fmnist_x_test, fmnist_y_test, 256, model)\n",
    "    print('Model {}, accuracy: {:.5f}'.format(i, acc))\n",
    "    fashion_mnist_acc.append(acc)\n",
    "    i += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}