{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159.1%C:\\Users\\itznu\\anaconda3\\envs\\data-science\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to cached_datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting cached_datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to cached_datasets\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to cached_datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting cached_datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to cached_datasets\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to cached_datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Extracting cached_datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to cached_datasets\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to cached_datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting cached_datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to cached_datasets\\FashionMNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = os.path.join('cached_datasets')\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(dataset_folder, train=True,\n",
    "                                           transform=None, target_transform=None, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(dataset_folder, train=False,\n",
    "                                          transform=None, target_transform=None, download=True)\n",
    "\n",
    "# Standardize data\n",
    "x_train = np.array(train_dataset.data, dtype=np.float)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
    "x_test = np.array(test_dataset.data, dtype=np.float)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
    "\n",
    "y_train = np.array(train_dataset.targets, dtype=np.int)\n",
    "y_test  = np.array(test_dataset.targets, dtype=np.int)\n",
    "\n",
    "# Network structure\n",
    "num_inputs = 28 * 28\n",
    "hidden_1_neurons = 400\n",
    "hidden_2_neurons = 100\n",
    "num_outputs = 10\n",
    "\n",
    "time_step = 1e-3\n",
    "num_steps = 100\n",
    "batch_size = 256\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\n",
    "    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\n",
    "\n",
    "    Args:\n",
    "    x -- The \"current\" values\n",
    "\n",
    "    Keyword args:\n",
    "    tau -- The membrane time constant of the LIF neuron to be charged\n",
    "    thr -- The firing threshold value\n",
    "    tmax -- The maximum time returned\n",
    "    epsilon -- A generic (small) epsilon > 0\n",
    "\n",
    "    Returns:\n",
    "    Time to first spike for each \"current\" x\n",
    "    \"\"\"\n",
    "    idx = x<thr\n",
    "    x = np.clip(x,thr+epsilon,1e9)\n",
    "    T = tau*np.log(x/(x-thr))\n",
    "    T[idx] = tmax\n",
    "    return T\n",
    "\n",
    "def sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\n",
    "    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=np.int)\n",
    "    number_of_batches = len(X)//batch_size\n",
    "    sample_index = np.arange(len(X))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    tau_eff = 20e-3/time_step\n",
    "    firing_times = np.array(current2firing_time(X, tau=tau_eff, tmax=nb_steps), dtype=np.int)\n",
    "    unit_numbers = np.arange(nb_units)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            c = firing_times[idx]<nb_steps\n",
    "            times, units = firing_times[idx][c], unit_numbers[c]\n",
    "\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index], device=device, dtype=torch.long)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-5.4615e-02, -2.6208e-02, -1.8990e-03, -9.8272e-02,  2.7721e-02,\n         -3.7305e-02, -4.7870e-02,  3.0612e-02, -1.0448e-01, -2.6787e-03],\n        [ 8.7552e-03,  1.2017e-01, -2.4053e-02,  1.0626e-03, -3.7044e-02,\n         -3.6515e-02,  9.3156e-02,  7.3120e-03,  7.7473e-02, -1.2317e-01],\n        [ 2.2835e-03, -3.8660e-02, -2.2359e-02, -6.6467e-02, -1.1967e-02,\n          7.6439e-02, -2.6147e-02, -4.1303e-02,  8.2683e-02,  1.6131e-02],\n        [-4.1177e-02, -1.4262e-02, -1.0349e-02, -4.3589e-03,  3.2278e-02,\n         -1.0232e-02, -7.7661e-02, -2.7474e-02, -1.5275e-02, -4.3727e-02],\n        [-5.0640e-03, -7.2224e-02, -5.3680e-02,  1.3708e-01,  2.7200e-02,\n          2.6782e-02, -1.4277e-01,  1.0471e-02, -6.4156e-02,  1.2150e-01],\n        [ 9.3228e-02, -1.3189e-02, -7.5031e-02, -8.5655e-02, -1.0981e-01,\n         -1.2461e-02, -2.2109e-03,  5.8592e-02,  1.7189e-02,  5.4864e-02],\n        [-6.0973e-02,  4.5472e-02, -1.1718e-01, -4.9780e-02,  4.7109e-02,\n          9.4628e-02,  9.0922e-02,  9.4810e-02,  3.7162e-02, -8.3260e-02],\n        [-4.8924e-02,  2.6713e-02,  6.0869e-02, -1.6529e-02,  5.2314e-02,\n          2.2025e-03, -3.1971e-02,  7.3556e-03, -8.2933e-03, -9.0909e-02],\n        [-2.0034e-02,  5.0162e-02, -2.3492e-02, -1.4934e-02,  5.5169e-02,\n          3.3773e-03, -8.3940e-03, -3.4932e-02, -9.7816e-02,  4.9354e-02],\n        [-6.9573e-03,  6.8857e-02, -5.8499e-02,  5.4032e-02,  5.1666e-02,\n         -9.5237e-02, -2.8507e-02,  1.7894e-02,  1.5420e-02,  3.1903e-02],\n        [-2.8950e-02, -8.2015e-03, -4.4751e-02, -2.7609e-02,  5.4828e-02,\n          9.8096e-02,  5.4920e-02,  3.3729e-02, -6.3289e-02, -6.2237e-03],\n        [-1.0121e-01,  8.3115e-02, -4.1968e-02, -8.5764e-02,  5.3876e-03,\n          2.6952e-02, -8.6301e-02, -4.1966e-02,  4.1269e-02, -1.8619e-02],\n        [ 8.1849e-03,  2.7098e-02, -2.8726e-02, -3.3457e-02,  4.8960e-02,\n         -5.1917e-02,  5.6302e-02, -7.1101e-02,  1.6846e-02, -6.4763e-02],\n        [-3.9983e-02, -7.1311e-02, -5.1081e-02, -5.0360e-02, -1.3058e-02,\n          8.0333e-02, -5.8367e-02, -1.5416e-01,  3.7298e-03,  2.4418e-02],\n        [ 3.1689e-02, -4.9768e-03, -1.2437e-02,  5.6466e-03, -1.4445e-02,\n          2.9850e-02,  4.8879e-02,  1.2527e-01,  1.8149e-03,  2.6594e-02],\n        [ 3.0576e-02, -9.6701e-02,  2.0878e-02,  8.1776e-03, -1.2833e-02,\n         -3.8292e-02, -1.4152e-02, -2.0282e-02, -9.7885e-02, -8.0090e-02],\n        [ 8.8756e-02,  6.7368e-02, -1.0537e-01,  4.3882e-02, -2.6797e-02,\n          1.4656e-02,  1.7503e-02, -1.4433e-01, -9.3915e-02, -3.2017e-03],\n        [-4.3187e-02, -2.5279e-02, -9.9641e-03,  2.8671e-02, -8.7803e-03,\n          4.4746e-02, -7.0511e-02,  1.0787e-01,  1.3766e-01, -5.6846e-02],\n        [ 3.7876e-02, -1.3121e-01,  5.5173e-02,  2.6548e-02,  1.1369e-02,\n          1.1042e-01, -3.0731e-02, -3.3161e-02, -7.8058e-04, -2.1738e-02],\n        [-9.8767e-02, -2.2079e-02,  3.2762e-02,  4.1213e-03, -1.1898e-02,\n         -8.9886e-02,  1.8919e-02, -1.5461e-02,  1.0650e-01, -5.7124e-02],\n        [ 8.6776e-02,  3.3648e-02,  1.6959e-02,  4.6687e-04, -5.6333e-02,\n         -5.6819e-02, -7.3378e-02, -2.1565e-02, -2.5404e-02, -7.3039e-02],\n        [-3.2312e-02,  1.8611e-01,  4.6295e-02, -1.0559e-01, -6.5888e-02,\n         -5.9391e-02, -4.9314e-02,  5.7725e-02, -1.2626e-01,  3.1709e-02],\n        [ 2.3336e-02, -7.1672e-02, -3.1243e-02, -1.1900e-01, -1.4868e-03,\n          1.3262e-02, -4.8843e-02, -1.1269e-01, -5.1770e-02, -1.0102e-02],\n        [-7.2164e-03, -9.6383e-02,  3.1832e-02,  5.5248e-02,  2.6988e-02,\n         -3.7446e-02, -4.3518e-02,  1.4950e-02,  4.5175e-02, -5.0331e-02],\n        [-8.9589e-02,  3.3596e-02,  1.4541e-01,  3.7894e-02,  5.2552e-03,\n          3.0012e-02,  7.7191e-02, -7.0729e-02, -7.7713e-02, -6.5578e-03],\n        [-6.7773e-02, -7.1808e-02, -5.4661e-02, -2.9949e-02,  5.2595e-02,\n         -1.5382e-01,  6.9307e-03,  2.7177e-02,  4.5784e-02,  1.3655e-02],\n        [ 3.6943e-02, -1.7409e-02, -2.7163e-02,  1.2412e-01, -3.9367e-02,\n          3.3532e-02, -9.4239e-03, -4.3029e-02,  1.4015e-02,  1.1459e-02],\n        [ 4.4999e-02, -1.5102e-02, -8.4468e-02, -2.7285e-03, -8.5857e-02,\n         -6.1574e-02,  6.1416e-02, -9.9129e-04,  5.5216e-02,  9.7400e-02],\n        [ 3.8382e-02,  7.5844e-02, -1.1456e-01, -1.4266e-01,  1.0952e-01,\n         -6.6389e-02,  6.1038e-02, -2.8972e-02, -4.9646e-02, -5.5474e-03],\n        [-2.5058e-02,  5.2174e-02,  4.4583e-03,  4.0096e-02,  9.9544e-03,\n          5.7366e-02, -1.7252e-03,  2.6030e-02,  6.2334e-02,  3.8392e-03],\n        [ 7.3059e-02, -3.9298e-02,  1.1417e-02, -1.9970e-02, -6.7346e-03,\n         -5.7374e-02, -6.6235e-02,  4.6432e-02, -7.1165e-02, -1.7206e-02],\n        [-3.7566e-02, -8.3176e-02,  3.9201e-02,  4.1831e-02,  3.4943e-02,\n         -1.4056e-02, -3.6368e-02,  1.0476e-03, -3.4524e-02,  8.0972e-02],\n        [ 1.2382e-02, -3.6331e-03,  3.2848e-02,  5.0498e-02,  6.4976e-02,\n          3.8213e-02, -4.0346e-02,  6.4624e-02, -2.7563e-02,  2.0846e-02],\n        [ 9.8348e-04,  3.1002e-03,  8.8493e-02, -2.0350e-02, -2.4696e-02,\n         -2.2582e-02,  7.1848e-02, -4.3196e-02, -2.1446e-03, -8.9766e-02],\n        [-3.2335e-02, -5.2701e-02,  1.1612e-01,  7.5629e-02, -8.5546e-02,\n          6.9452e-03,  5.0621e-02,  4.7209e-02, -1.5846e-02,  7.9209e-02],\n        [ 7.5562e-02,  1.2767e-02, -6.2025e-02, -7.1460e-03, -4.5287e-02,\n          2.5885e-03,  1.3649e-02, -5.3957e-02, -4.3006e-02,  1.9368e-02],\n        [-8.8717e-02,  1.0676e-02, -5.3609e-02,  9.0948e-02,  3.8712e-02,\n          5.6578e-02, -2.8996e-04,  2.7374e-02,  2.3508e-02,  2.6354e-02],\n        [ 4.1929e-02,  7.5256e-02, -5.3267e-02, -8.3998e-02,  4.5852e-02,\n          3.7466e-02,  5.4199e-03,  5.8645e-03,  1.2311e-01,  1.5447e-02],\n        [-9.2810e-02,  2.5687e-02, -6.2582e-02, -2.7203e-02, -8.0157e-02,\n          5.6527e-02,  2.4294e-02,  4.5138e-02,  1.0187e-01, -1.3261e-01],\n        [-8.7145e-03, -4.1701e-02, -7.5259e-02, -1.0242e-01, -6.6845e-02,\n         -3.7650e-02, -6.2435e-02, -4.5608e-02, -5.7627e-02, -5.3696e-02],\n        [ 2.9358e-02,  2.9605e-02,  1.8189e-02, -4.5201e-02,  2.0822e-02,\n          1.0703e-01,  6.2857e-03, -3.6528e-03,  1.5732e-02, -3.9680e-02],\n        [-5.0768e-02,  1.6791e-02, -2.0377e-02, -3.7220e-02, -8.0891e-02,\n          1.7925e-02,  3.2213e-04,  3.4124e-03, -7.9863e-02, -5.2215e-02],\n        [-9.0896e-03, -1.5950e-02, -5.1500e-02, -8.1845e-02, -6.3388e-02,\n         -5.8749e-03, -2.5729e-02, -4.9480e-02, -1.3851e-02,  9.3574e-04],\n        [-1.1446e-02, -1.3218e-02,  3.8153e-02, -9.9317e-02,  5.9434e-03,\n         -4.7710e-02,  3.5715e-03, -3.2144e-02,  5.0519e-02,  3.2269e-02],\n        [ 3.0135e-02, -8.2063e-02, -4.9039e-02, -5.7317e-02,  1.1447e-02,\n          6.6599e-02,  5.4856e-04, -6.4986e-03,  7.2037e-02, -4.5891e-02],\n        [-3.3423e-02,  2.2256e-02, -1.1845e-02,  3.5614e-02, -3.8747e-03,\n         -7.4073e-03, -7.9426e-02, -9.9804e-02,  9.3101e-02,  5.6063e-02],\n        [ 4.7679e-03, -5.0949e-02, -8.4322e-02,  1.1322e-01, -2.9071e-02,\n          1.3656e-02,  1.3042e-01,  4.6341e-02, -5.2475e-02,  4.3729e-02],\n        [ 6.5442e-02,  5.6203e-02,  2.1414e-02, -9.2421e-02, -8.5728e-02,\n         -9.5754e-02,  8.6925e-02, -6.9067e-02,  4.4176e-02, -4.5435e-02],\n        [ 4.3875e-02,  2.1510e-02,  3.4099e-03, -6.3916e-02,  2.1943e-02,\n         -4.4903e-02,  3.3384e-02, -3.0615e-02,  8.6262e-03, -3.9189e-02],\n        [ 5.6117e-02, -7.1925e-02,  1.0112e-02,  7.6493e-02, -3.5554e-02,\n          3.5448e-02, -2.5330e-02,  1.3750e-02, -4.9874e-03,  2.4038e-02],\n        [ 4.6296e-02, -2.7400e-02, -1.7355e-02,  5.3632e-03, -5.2903e-02,\n          4.7415e-02,  3.6945e-02, -1.3614e-02,  3.3591e-02, -2.0459e-02],\n        [ 3.5209e-02,  2.8298e-03, -5.9418e-03,  4.1865e-02, -6.0453e-02,\n          7.4811e-02, -1.6279e-02,  4.2609e-02,  2.6130e-02,  4.1236e-02],\n        [-1.6852e-02, -1.0131e-01, -1.1558e-01,  4.7879e-02,  2.2720e-02,\n          7.1617e-02, -8.8809e-03, -5.5184e-02, -1.9183e-03, -1.4971e-02],\n        [-9.6971e-03, -8.8034e-02,  2.9681e-02,  6.5695e-02, -6.5377e-02,\n         -7.6978e-02,  7.7557e-02,  3.7980e-02, -5.7317e-02,  1.1271e-01],\n        [ 8.5870e-02, -3.0505e-02, -3.5195e-02,  3.3546e-02,  4.3537e-02,\n          1.8402e-02, -2.4393e-02, -7.0611e-02,  2.8179e-02, -5.2870e-02],\n        [ 2.0809e-02,  7.3646e-02, -1.3877e-01,  2.1137e-02, -9.7235e-02,\n          2.7983e-02, -3.9526e-02, -1.8139e-02, -1.1603e-02,  3.5869e-02],\n        [ 1.8727e-02,  9.2257e-02,  1.9511e-02, -9.0117e-02, -2.4965e-02,\n          4.9461e-02,  7.6122e-02,  2.4275e-02,  4.7902e-02, -1.5836e-02],\n        [-2.7736e-02,  5.9640e-02, -5.8286e-03,  4.7740e-02,  5.7804e-03,\n          6.7722e-02,  1.3923e-02,  5.4888e-02, -2.0091e-03, -2.7879e-02],\n        [ 4.3705e-02, -7.1469e-02, -3.2952e-02, -2.3631e-02,  3.1606e-02,\n          1.5275e-02,  1.7163e-02,  3.7366e-02,  7.3157e-02,  7.2962e-02],\n        [-1.6798e-01, -7.5078e-04,  3.5304e-02, -6.1128e-02, -1.4104e-02,\n         -2.8932e-02, -7.9855e-03,  6.1286e-03, -2.0208e-02, -6.3539e-03],\n        [-3.4639e-02, -9.4310e-03,  1.7592e-02,  9.4102e-03,  8.9163e-03,\n         -1.4921e-02, -4.0720e-02,  5.0723e-03, -1.1021e-01, -8.0721e-02],\n        [ 1.8858e-02,  5.1680e-02,  1.0401e-01,  5.2690e-02,  3.9293e-02,\n         -4.1833e-02,  1.0198e-02,  2.2051e-04,  6.4227e-02, -2.5727e-02],\n        [-4.3084e-02, -4.2445e-02,  2.9474e-02, -6.1514e-02, -1.0836e-01,\n         -9.1032e-02,  3.4264e-02,  4.4124e-03,  2.0271e-02,  1.0806e-02],\n        [ 8.1104e-02,  7.8524e-02,  2.5009e-02,  3.2212e-02,  6.5509e-02,\n          5.4285e-02, -3.3111e-02, -9.7781e-02, -8.1424e-02, -9.1145e-03],\n        [ 3.7979e-02,  7.6817e-03,  1.3140e-02, -1.0087e-01,  1.1238e-02,\n         -3.2578e-03,  2.0039e-02, -9.5390e-03, -4.9545e-02, -2.8143e-02],\n        [-6.7522e-02, -3.3981e-02, -2.3044e-02, -6.5105e-03, -5.0335e-02,\n          5.6850e-02, -2.0533e-02, -1.6208e-02, -1.1897e-03, -9.5075e-03],\n        [-1.2824e-01,  7.4393e-02, -7.6918e-02,  6.7294e-02, -3.1185e-02,\n         -5.3485e-03,  4.0418e-03,  5.5283e-03, -4.2176e-02,  4.8787e-02],\n        [ 1.0149e-02, -1.7372e-02,  1.7363e-02, -8.1846e-02,  8.7634e-03,\n         -4.2622e-02, -4.9614e-02,  1.5529e-02, -4.2908e-04,  1.8814e-02],\n        [-6.4798e-02,  2.1341e-02, -4.0062e-02, -4.2632e-02, -6.8720e-02,\n          2.8820e-02, -6.3495e-03,  3.5705e-03,  4.4223e-02,  8.8473e-02],\n        [-2.1353e-02, -4.1553e-02,  5.9520e-03, -9.8869e-03, -8.8052e-02,\n         -6.9524e-02, -1.3776e-02,  3.9692e-02, -2.6868e-02, -2.0947e-02],\n        [-2.1882e-02,  3.6432e-02, -6.7299e-02,  1.0548e-01, -1.6881e-02,\n         -1.5808e-01,  4.5528e-02,  1.2881e-02,  1.8630e-04, -4.5029e-02],\n        [ 2.0845e-02,  5.5849e-04, -3.4157e-02,  9.2493e-02, -6.3946e-02,\n         -4.6856e-02, -5.3575e-02, -5.9350e-02,  2.4373e-02, -1.6006e-02],\n        [ 8.5794e-02,  2.3421e-02, -7.2635e-03,  2.5714e-02,  5.8266e-02,\n          2.7132e-02,  1.5033e-02, -8.0717e-02,  4.1298e-02,  9.1900e-03],\n        [ 2.0993e-02,  1.0109e-01, -4.7875e-02, -4.1073e-02, -1.3325e-01,\n         -2.7250e-03,  3.9205e-02,  3.5598e-02,  4.6297e-02, -6.5074e-02],\n        [ 5.4233e-02, -6.6995e-02,  1.4443e-01, -5.1411e-03,  6.8867e-02,\n         -8.9328e-02,  2.0722e-02, -1.8464e-02,  2.5253e-02,  6.1356e-02],\n        [-1.3143e-01,  8.5331e-02, -3.4877e-02,  9.0839e-02, -3.3680e-02,\n          1.9206e-02,  4.7073e-02,  2.7295e-02, -4.8044e-02, -1.9520e-02],\n        [ 1.3726e-02, -4.3412e-02,  4.6660e-02, -1.6519e-02,  3.1976e-02,\n          1.5547e-02,  7.5184e-02,  6.5109e-02,  1.4680e-02,  4.2679e-02],\n        [-5.6447e-02, -1.8238e-02, -2.9191e-02,  4.3143e-02,  5.9496e-02,\n         -8.3925e-03,  3.5799e-02,  2.5798e-02,  1.2749e-02, -4.8257e-02],\n        [ 6.2008e-02,  1.3861e-02,  4.1379e-02,  2.8165e-02,  1.4892e-02,\n         -5.3155e-02,  3.6452e-02, -3.1385e-02,  9.5550e-02, -1.4324e-02],\n        [-1.4601e-02, -2.9701e-03,  1.0941e-02, -3.1714e-02,  2.7458e-02,\n          4.6287e-02,  3.2995e-02,  7.5855e-04,  5.6916e-02, -3.4703e-02],\n        [ 9.1654e-02,  2.0734e-03, -1.2129e-01, -1.0571e-02,  6.9121e-03,\n          1.3244e-02,  3.7507e-02, -4.2496e-02, -3.2232e-02, -2.5492e-02],\n        [-1.1675e-01, -9.3826e-02,  4.5425e-02,  4.9146e-03,  7.6721e-02,\n          1.0737e-01,  7.0215e-02, -1.5689e-02, -4.3816e-02, -2.7899e-02],\n        [-2.1954e-01, -4.4919e-02,  7.5515e-02,  6.1066e-02,  1.6741e-02,\n         -4.9489e-03,  6.6020e-02,  4.9807e-02, -1.6336e-03,  2.1816e-02],\n        [-5.1848e-02, -7.3988e-02, -1.6484e-02, -6.3898e-02,  9.1850e-03,\n          3.4968e-02,  7.5752e-02, -1.3638e-01, -7.8518e-02,  7.6199e-02],\n        [-3.4812e-02,  1.2353e-01, -4.1703e-03, -1.3654e-02, -9.7143e-02,\n          2.5837e-02, -3.5469e-02, -4.5148e-02,  4.1011e-02,  2.9149e-02],\n        [ 1.7020e-02,  1.7492e-02,  8.3545e-02,  1.2806e-01,  4.6368e-02,\n          3.9848e-02, -1.6645e-02, -1.4813e-02, -5.1705e-02, -1.1814e-01],\n        [ 1.1547e-01,  2.1579e-03, -1.2389e-02,  3.2314e-03,  3.4864e-02,\n          2.8654e-02,  8.3101e-03,  8.0313e-03, -2.2724e-02,  1.7438e-02],\n        [ 3.2428e-04, -2.4292e-02,  1.0281e-02,  2.0496e-02,  2.4964e-02,\n          2.3603e-02,  3.1762e-02, -4.3094e-02, -5.1049e-02,  3.0610e-02],\n        [-1.8555e-03,  5.7889e-02,  4.7374e-02,  2.3131e-02,  5.0834e-03,\n          1.1470e-01, -4.3856e-02,  7.3821e-02, -9.9350e-02, -1.2498e-02],\n        [ 4.7725e-02, -1.3833e-02, -6.3798e-03,  6.7025e-02, -1.9824e-03,\n          7.8565e-02, -6.1305e-02,  5.9034e-02,  6.0000e-02,  4.0815e-02],\n        [ 5.9426e-02,  6.0774e-02,  7.7571e-02, -4.5704e-02,  3.8584e-03,\n         -2.7444e-03,  1.8100e-02,  5.5539e-02, -4.2496e-02, -8.7399e-02],\n        [ 6.6196e-02, -8.7874e-02,  8.4683e-04,  7.8105e-02,  2.0939e-02,\n          1.4658e-01, -2.0848e-02, -1.0189e-01,  1.3310e-02, -4.4779e-03],\n        [ 1.0171e-01,  7.7297e-02,  4.0243e-02,  3.5394e-02,  1.1031e-03,\n         -1.9927e-02, -1.0792e-02, -4.6779e-02,  7.0344e-02, -5.5771e-03],\n        [ 3.8220e-02,  9.4936e-03, -4.3908e-02, -1.7293e-03,  2.8351e-02,\n         -1.4501e-02,  3.5841e-02,  7.7476e-02, -9.6054e-02, -1.2155e-02],\n        [-7.5294e-03,  4.9771e-02, -4.6192e-03, -4.5111e-02,  2.7871e-03,\n          1.9243e-02,  3.8032e-02,  6.9573e-02,  1.1936e-02, -1.9570e-02],\n        [-1.2470e-01,  5.8118e-02, -8.1639e-02,  1.8650e-01, -5.3911e-03,\n          6.8945e-02,  2.0317e-02,  1.7846e-02,  2.6445e-02, -2.7686e-02],\n        [ 4.1043e-02, -7.6065e-02, -3.7915e-02,  2.9962e-02,  6.9364e-02,\n         -5.8743e-02,  2.1999e-02, -1.2145e-01, -2.0198e-02, -3.3619e-02],\n        [-1.0576e-01,  9.2362e-02,  2.9580e-02, -5.5487e-02,  6.3536e-02,\n          1.0811e-01,  5.2428e-02,  1.3338e-02,  4.6777e-04, -4.3190e-02],\n        [ 4.2328e-02, -3.4840e-02, -8.5054e-02, -1.0521e-02, -5.1289e-02,\n          2.0734e-02, -2.3101e-02,  5.3348e-02, -1.2735e-01, -4.3797e-02],\n        [-4.5793e-02,  7.2838e-03, -1.8246e-02, -4.3774e-02,  6.0171e-02,\n          1.6504e-02,  1.2594e-02, -6.3147e-02, -2.4976e-02,  4.9604e-02]],\n       device='cuda:0', requires_grad=True)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization of the network\n",
    "weight_scale = 6 * (1.0-beta)\n",
    "w1 = torch.empty((num_inputs, hidden_1_neurons), device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.empty((hidden_1_neurons, hidden_2_neurons), device=device, dtype=dtype, requires_grad=True)\n",
    "w3 = torch.empty((hidden_2_neurons, num_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale / np.sqrt(num_inputs))\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale / np.sqrt(hidden_1_neurons))\n",
    "torch.nn.init.normal_(w3, mean=0.0, std=weight_scale / np.sqrt(hidden_2_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def run_snn(inputs):\n",
    "    h1 = torch.einsum('abc,cd->abd', (inputs, w1))\n",
    "    syn_hidden_1 = torch.zeros((batch_size, hidden_1_neurons), device=device, dtype=dtype)\n",
    "    mem_hidden_1 = torch.zeros((batch_size, hidden_1_neurons), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec_hidden_1 = [mem_hidden_1]\n",
    "    spike_rec_hidden_1 = [mem_hidden_1]\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        mem_threshold = mem_hidden_1 - 1.0\n",
    "        out = spike_fn(mem_threshold)\n",
    "        rst = torch.zeros_like(mem_hidden_1)\n",
    "        c = (mem_threshold > 0)\n",
    "        rst[c] = torch.ones_like(mem_hidden_1)[c]\n",
    "\n",
    "        new_syn = alpha * syn_hidden_1 + h1[:, t]\n",
    "        new_mem = beta * mem_hidden_1 + syn_hidden_1 - rst\n",
    "        mem_hidden_1 = new_mem\n",
    "        syn_hidden_1 = new_syn\n",
    "\n",
    "        mem_rec_hidden_1.append(mem_hidden_1)\n",
    "        spike_rec_hidden_1.append(out)\n",
    "\n",
    "    spike_rec_hidden_1 = torch.stack(spike_rec_hidden_1, dim=1)\n",
    "    mem_rec_hidden_1 = torch.stack(mem_rec_hidden_1, dim=1)\n",
    "\n",
    "    h2 = torch.einsum('abc,cd->abd', (spike_rec_hidden_1, w2))\n",
    "    syn_hidden_2 = torch.zeros((batch_size, hidden_2_neurons), device=device, dtype=dtype)\n",
    "    mem_hidden_2 = torch.zeros((batch_size, hidden_2_neurons), device=device, dtype=dtype)\n",
    "\n",
    "    mem_2_rec = [mem_hidden_2]\n",
    "    spike_2_rec = [mem_hidden_2]\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        mem_threshold = mem_hidden_2 - 1.0\n",
    "        out = spike_fn(mem_threshold)\n",
    "        rst = torch.zeros_like(mem_hidden_2)\n",
    "        c = (mem_threshold > 0)\n",
    "        rst[c] = torch.ones_like(mem_hidden_2)[c]\n",
    "\n",
    "        new_syn = alpha * syn_hidden_2 + h2[:, t]\n",
    "        new_mem = beta * mem_hidden_2 + syn_hidden_2 - rst\n",
    "        mem_hidden_2 = new_mem\n",
    "        syn_hidden_2 = new_syn\n",
    "\n",
    "        mem_2_rec.append(mem_hidden_2)\n",
    "        spike_2_rec.append(out)\n",
    "\n",
    "    mem_2_rec = torch.stack(mem_2_rec, dim=1)\n",
    "    spike_2_rec = torch.stack(spike_2_rec, dim=1)\n",
    "\n",
    "    h3 = torch.einsum('abc,cd->abd', (spike_2_rec, w3))\n",
    "    flt = torch.zeros((batch_size, num_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size, num_outputs), device=device, dtype=dtype)\n",
    "\n",
    "    out_rec = [out]\n",
    "    for t in range(num_steps):\n",
    "        new_flt = alpha * flt + h3[:, t]\n",
    "        new_out = beta * out + flt\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "        out_rec.append(out)\n",
    "    \n",
    "    out_rec = torch.stack(out_rec, dim=1)\n",
    "    prev_recs = [[mem_rec_hidden_1, spike_rec_hidden_1], [mem_2_rec, spike_2_rec]]\n",
    "\n",
    "    return out_rec, prev_recs\n",
    "\n",
    "def train(x_data, y_data, lr=1e-3, nb_epochs=10):\n",
    "    params = [w1, w2, w3]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    loss_hist = []\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, num_inputs):\n",
    "            output, _ = run_snn(x_local.to_dense())\n",
    "            m, _ = torch.max(output, 1)\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "            loss_val = loss_fn(log_p_y, y_local)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        print(\"Epoch %i: loss=%.5f\"%(epoch+1,mean_loss))\n",
    "        loss_hist.append(mean_loss)\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "def compute_classification_accuracy(x_data, y_data):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, num_inputs, shuffle=False):\n",
    "        output,_ = run_snn(x_local.to_dense())\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.84375\n",
      "Epoch 2: loss=0.51504\n",
      "Epoch 3: loss=0.46160\n",
      "Epoch 4: loss=0.43026\n",
      "Epoch 5: loss=0.40678\n",
      "Epoch 6: loss=0.39004\n",
      "Epoch 7: loss=0.37078\n",
      "Epoch 8: loss=0.35987\n",
      "Epoch 9: loss=0.35034\n",
      "Epoch 10: loss=0.34012\n",
      "Training accuracy: 0.88118\n",
      "Test accuracy: 0.85667\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 495x300 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loss_hist = train(x_train, y_train, lr=2e-4, nb_epochs=10)\n",
    "plt.figure(figsize=(3.3,2),dpi=150)\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "sns.despine()\n",
    "\n",
    "print(\"Training accuracy: %.5f\"%(compute_classification_accuracy(x_train,y_train)))\n",
    "print(\"Test accuracy: %.5f\"%(compute_classification_accuracy(x_test,y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9c9070f4",
   "language": "python",
   "display_name": "PyCharm (use-of-snn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}