{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join('cached_datasets')\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(dataset_folder, train=True,\n",
    "                                           transform=None, target_transform=None, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(dataset_folder, train=False,\n",
    "                                          transform=None, target_transform=None, download=True)\n",
    "\n",
    "# Standardize data\n",
    "x_train = np.array(train_dataset.data, dtype=np.float)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
    "x_test = np.array(test_dataset.data, dtype=np.float)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
    "\n",
    "y_train = np.array(train_dataset.targets, dtype=np.int)\n",
    "y_test  = np.array(test_dataset.targets, dtype=np.int)\n",
    "\n",
    "# Network structure\n",
    "num_inputs = 28 * 28\n",
    "hidden_1_neurons = 100\n",
    "hidden_2_neurons = 150\n",
    "hidden_3_neurons = 250\n",
    "num_outputs = 10\n",
    "\n",
    "time_step = 1e-3\n",
    "num_steps = 100\n",
    "batch_size = 256\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\n",
    "    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\n",
    "\n",
    "    Args:\n",
    "    x -- The \"current\" values\n",
    "\n",
    "    Keyword args:\n",
    "    tau -- The membrane time constant of the LIF neuron to be charged\n",
    "    thr -- The firing threshold value\n",
    "    tmax -- The maximum time returned\n",
    "    epsilon -- A generic (small) epsilon > 0\n",
    "\n",
    "    Returns:\n",
    "    Time to first spike for each \"current\" x\n",
    "    \"\"\"\n",
    "    idx = x<thr\n",
    "    x = np.clip(x,thr+epsilon,1e9)\n",
    "    T = tau*np.log(x/(x-thr))\n",
    "    T[idx] = tmax\n",
    "    return T\n",
    "\n",
    "def sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\n",
    "    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=np.int)\n",
    "    number_of_batches = len(X)//batch_size\n",
    "    sample_index = np.arange(len(X))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    tau_eff = 20e-3/time_step\n",
    "    firing_times = np.array(current2firing_time(X, tau=tau_eff, tmax=nb_steps), dtype=np.int)\n",
    "    unit_numbers = np.arange(nb_units)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            c = firing_times[idx]<nb_steps\n",
    "            times, units = firing_times[idx][c], unit_numbers[c]\n",
    "\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index], device=device, dtype=torch.long)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0234, -0.0405,  0.0211,  ..., -0.0343,  0.0109, -0.0098],\n        [ 0.0706,  0.0010, -0.0412,  ..., -0.0040,  0.0269, -0.0514],\n        [-0.0045,  0.0050, -0.0501,  ..., -0.0386, -0.0474, -0.0347],\n        ...,\n        [-0.0272,  0.0180, -0.0315,  ...,  0.0373, -0.0205,  0.0099],\n        [-0.0086,  0.0171, -0.0073,  ..., -0.0102,  0.0378,  0.0154],\n        [-0.0437, -0.0589, -0.0545,  ...,  0.0380,  0.0066,  0.0264]],\n       device='cuda:0', requires_grad=True)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization of the network\n",
    "weight_scale = 6 * (1.0-beta)\n",
    "w1 = torch.empty((num_inputs, hidden_1_neurons), device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.empty((hidden_1_neurons, hidden_2_neurons), device=device, dtype=dtype, requires_grad=True)\n",
    "w3 = torch.empty((hidden_2_neurons, hidden_3_neurons), device=device, dtype=dtype, requires_grad=True)\n",
    "w4 = torch.empty((hidden_3_neurons, num_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale / np.sqrt(num_inputs))\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale / np.sqrt(hidden_1_neurons))\n",
    "torch.nn.init.normal_(w3, mean=0.0, std=weight_scale / np.sqrt(hidden_2_neurons))\n",
    "torch.nn.init.normal_(w4, mean=0.0, std=weight_scale / np.sqrt(hidden_3_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def run_snn(inputs):\n",
    "    h1 = torch.einsum('abc,cd->abd', (inputs, w1))\n",
    "    syn_hidden_1 = torch.zeros((batch_size, hidden_1_neurons), device=device, dtype=dtype)\n",
    "    mem_hidden_1 = torch.zeros((batch_size, hidden_1_neurons), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec_hidden_1 = [mem_hidden_1]\n",
    "    spike_rec_hidden_1 = [mem_hidden_1]\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        mem_threshold = mem_hidden_1 - 1.0\n",
    "        out = spike_fn(mem_threshold)\n",
    "        rst = torch.zeros_like(mem_hidden_1)\n",
    "        c = (mem_threshold > 0)\n",
    "        rst[c] = torch.ones_like(mem_hidden_1)[c]\n",
    "\n",
    "        new_syn = alpha * syn_hidden_1 + h1[:, t]\n",
    "        new_mem = beta * mem_hidden_1 + syn_hidden_1 - rst\n",
    "        mem_hidden_1 = new_mem\n",
    "        syn_hidden_1 = new_syn\n",
    "\n",
    "        mem_rec_hidden_1.append(mem_hidden_1)\n",
    "        spike_rec_hidden_1.append(out)\n",
    "\n",
    "    spike_rec_hidden_1 = torch.stack(spike_rec_hidden_1, dim=1)\n",
    "    mem_rec_hidden_1 = torch.stack(mem_rec_hidden_1, dim=1)\n",
    "\n",
    "    h2 = torch.einsum('abc,cd->abd', (spike_rec_hidden_1, w2))\n",
    "    syn_hidden_2 = torch.zeros((batch_size, hidden_2_neurons), device=device, dtype=dtype)\n",
    "    mem_hidden_2 = torch.zeros((batch_size, hidden_2_neurons), device=device, dtype=dtype)\n",
    "\n",
    "    mem_2_rec = [mem_hidden_2]\n",
    "    spike_2_rec = [mem_hidden_2]\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        mem_threshold = mem_hidden_2 - 1.0\n",
    "        out = spike_fn(mem_threshold)\n",
    "        rst = torch.zeros_like(mem_hidden_2)\n",
    "        c = (mem_threshold > 0)\n",
    "        rst[c] = torch.ones_like(mem_hidden_2)[c]\n",
    "\n",
    "        new_syn = alpha * syn_hidden_2 + h2[:, t]\n",
    "        new_mem = beta * mem_hidden_2 + syn_hidden_2 - rst\n",
    "        mem_hidden_2 = new_mem\n",
    "        syn_hidden_2 = new_syn\n",
    "\n",
    "        mem_2_rec.append(mem_hidden_2)\n",
    "        spike_2_rec.append(out)\n",
    "\n",
    "    mem_2_rec = torch.stack(mem_2_rec, dim=1)\n",
    "    spike_2_rec = torch.stack(spike_2_rec, dim=1)\n",
    "\n",
    "    h3 = torch.einsum('abc,cd->abd', (spike_2_rec, w3))\n",
    "    syn_hidden_3 = torch.zeros((batch_size, hidden_3_neurons), device=device, dtype=dtype)\n",
    "    mem_hidden_3 = torch.zeros((batch_size, hidden_3_neurons), device=device, dtype=dtype)\n",
    "\n",
    "    mem_3_rec = [mem_hidden_3]\n",
    "    spike_3_rec = [mem_hidden_3]\n",
    "    for t in range(num_steps):\n",
    "        mem_threshold = mem_hidden_3 - 1.0\n",
    "        out = spike_fn(mem_threshold)\n",
    "        rst = torch.zeros_like(mem_hidden_3)\n",
    "        c = (mem_threshold > 0)\n",
    "        rst[c] = torch.ones_like(mem_hidden_3)[c]\n",
    "\n",
    "        new_syn = alpha * syn_hidden_3 + h3[:, t]\n",
    "        new_mem = beta * mem_hidden_3 + syn_hidden_3 - rst\n",
    "        mem_hidden_3 = new_mem\n",
    "        syn_hidden_3 = new_syn\n",
    "\n",
    "        mem_3_rec.append(mem_hidden_3)\n",
    "        spike_3_rec.append(out)\n",
    "\n",
    "    mem_3_rec = torch.stack(mem_3_rec, dim=1)\n",
    "    spike_3_rec = torch.stack(spike_3_rec, dim=1)\n",
    "\n",
    "    h4 = torch.einsum('abc,cd->abd',(spike_3_rec, w4))\n",
    "    flt = torch.zeros((batch_size, num_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size, num_outputs), device=device, dtype=dtype)\n",
    "\n",
    "    out_rec = [out]\n",
    "    for t in range(num_steps):\n",
    "        new_flt = alpha * flt + h4[:, t]\n",
    "        new_out = beta * out + flt\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "        out_rec.append(out)\n",
    "    \n",
    "    out_rec = torch.stack(out_rec, dim=1)\n",
    "    prev_recs = [[mem_rec_hidden_1, spike_rec_hidden_1], [mem_2_rec, spike_2_rec], [mem_3_rec, spike_3_rec]]\n",
    "\n",
    "    return out_rec, prev_recs\n",
    "\n",
    "def train(x_data, y_data, lr=1e-3, nb_epochs=10):\n",
    "    params = [w1, w2, w3, w4]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    loss_hist = []\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, num_inputs):\n",
    "            output, _ = run_snn(x_local.to_dense())\n",
    "            m, _ = torch.max(output, 1)\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "            loss_val = loss_fn(log_p_y, y_local)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        print(\"Epoch %i: loss=%.5f\"%(epoch+1,mean_loss))\n",
    "        loss_hist.append(mean_loss)\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "def compute_classification_accuracy(x_data, y_data):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, num_inputs, shuffle=False):\n",
    "        output,_ = run_snn(x_local.to_dense())\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=1.09013\n",
      "Epoch 2: loss=0.68389\n",
      "Epoch 3: loss=0.57984\n",
      "Epoch 4: loss=0.53690\n",
      "Epoch 5: loss=0.50327\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-c25d8d3dee7f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mloss_hist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2e-4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnb_epochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfigsize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3.3\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdpi\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m150\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss_hist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mxlabel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Epoch\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mylabel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Loss\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-23-c1bacd5be370>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(x_data, y_data, lr, nb_epochs)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[0mloss_val\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[0mlocal_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss_val\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\data-science\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[0;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[1;32m--> 221\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\data-science\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[0;32m    128\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 130\u001B[1;33m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[0;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "loss_hist = train(x_train, y_train, lr=2e-4, nb_epochs=10)\n",
    "plt.figure(figsize=(3.3,2),dpi=150)\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "sns.despine()\n",
    "\n",
    "print(\"Training accuracy: %.5f\"%(compute_classification_accuracy(x_train,y_train)))\n",
    "print(\"Test accuracy: %.5f\"%(compute_classification_accuracy(x_test,y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9c9070f4",
   "language": "python",
   "display_name": "PyCharm (use-of-snn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}