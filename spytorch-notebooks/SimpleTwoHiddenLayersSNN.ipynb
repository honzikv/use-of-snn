{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join('cached_datasets')\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(dataset_folder, train=True,\n",
    "                                           transform=None, target_transform=None, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(dataset_folder, train=False,\n",
    "                                          transform=None, target_transform=None, download=True)\n",
    "\n",
    "# Standardize data\n",
    "x_train = np.array(train_dataset.data, dtype=np.float)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
    "x_test = np.array(test_dataset.data, dtype=np.float)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
    "\n",
    "y_train = np.array(train_dataset.targets, dtype=np.int)\n",
    "y_test  = np.array(test_dataset.targets, dtype=np.int)\n",
    "\n",
    "# Network structure\n",
    "num_inputs = 28 * 28\n",
    "hidden_1_neurons = 400\n",
    "hidden_2_neurons = 100\n",
    "num_outputs = 10\n",
    "\n",
    "time_step = 1e-3\n",
    "num_steps = 100\n",
    "batch_size = 256\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\n",
    "    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\n",
    "\n",
    "    Args:\n",
    "    x -- The \"current\" values\n",
    "\n",
    "    Keyword args:\n",
    "    tau -- The membrane time constant of the LIF neuron to be charged\n",
    "    thr -- The firing threshold value\n",
    "    tmax -- The maximum time returned\n",
    "    epsilon -- A generic (small) epsilon > 0\n",
    "\n",
    "    Returns:\n",
    "    Time to first spike for each \"current\" x\n",
    "    \"\"\"\n",
    "    idx = x<thr\n",
    "    x = np.clip(x,thr+epsilon,1e9)\n",
    "    T = tau*np.log(x/(x-thr))\n",
    "    T[idx] = tmax\n",
    "    return T\n",
    "\n",
    "def sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\n",
    "    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=np.int)\n",
    "    number_of_batches = len(X)//batch_size\n",
    "    sample_index = np.arange(len(X))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    tau_eff = 20e-3/time_step\n",
    "    firing_times = np.array(current2firing_time(X, tau=tau_eff, tmax=nb_steps), dtype=np.int)\n",
    "    unit_numbers = np.arange(nb_units)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            c = firing_times[idx]<nb_steps\n",
    "            times, units = firing_times[idx][c], unit_numbers[c]\n",
    "\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index], device=device, dtype=torch.long)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.3489e-02,  8.3269e-02,  2.5571e-02,  1.8164e-02,  1.0908e-02,\n         -4.9878e-03, -1.7014e-02, -4.9253e-03, -7.4427e-02, -3.8959e-02],\n        [ 2.4407e-02, -1.2090e-02, -4.8193e-02,  1.0894e-01, -5.1321e-03,\n          9.0793e-02,  1.2090e-02, -5.7131e-02, -1.7475e-03, -5.1820e-02],\n        [ 3.1207e-02, -5.9804e-02,  1.7869e-02, -2.9305e-02, -8.5588e-02,\n         -2.7671e-02,  1.5863e-02, -2.3803e-03,  6.4239e-03, -8.6702e-03],\n        [ 8.2120e-02,  1.0338e-01, -6.2238e-02,  6.2482e-02, -1.1227e-01,\n          5.6116e-02, -1.5692e-04, -4.2842e-02, -1.1731e-01,  1.9059e-03],\n        [-1.6282e-02,  3.2525e-02, -2.0130e-02,  8.3077e-02, -1.3067e-02,\n          1.1887e-02,  9.6926e-02,  5.3232e-02,  3.9386e-02,  6.7868e-02],\n        [ 1.6466e-03, -9.1754e-02,  5.1994e-02, -5.9243e-02, -1.4707e-01,\n         -2.8542e-03,  4.4251e-02, -2.2252e-02,  1.2515e-01, -3.6349e-04],\n        [ 6.2400e-02,  6.9580e-03,  8.6596e-02,  1.2261e-01, -6.4640e-02,\n          3.5454e-02, -2.4866e-02, -6.1034e-02,  1.0567e-01, -6.8375e-02],\n        [ 6.4302e-03,  6.1074e-02,  1.1654e-02, -4.1452e-02, -7.3510e-02,\n          1.0943e-01,  3.7013e-02, -5.2772e-02, -1.0466e-01,  3.0462e-02],\n        [-3.5682e-02, -8.1100e-03,  9.7449e-02, -5.9584e-02, -3.1313e-02,\n         -9.0520e-02,  3.6798e-02,  1.0632e-01, -1.3103e-02,  1.4455e-01],\n        [ 6.9074e-02, -3.3881e-02,  4.8341e-02,  2.0909e-02, -3.4173e-02,\n          5.6734e-02,  5.3084e-02,  5.6665e-02,  1.0351e-01,  6.4711e-02],\n        [ 2.1681e-02, -6.7579e-02, -1.1850e-01, -6.3258e-02, -5.0266e-02,\n         -9.9661e-02, -6.6753e-02,  1.0889e-01,  5.6859e-02,  1.4209e-02],\n        [-3.6236e-02,  1.2876e-01,  2.5051e-02, -1.1214e-02, -1.8030e-02,\n          9.0866e-02, -7.0200e-02, -8.0333e-02, -5.1202e-02, -1.0744e-01],\n        [ 7.2631e-02, -5.6529e-02,  2.7277e-02, -8.4212e-02, -1.1493e-02,\n          1.4061e-02, -4.7525e-02,  2.8646e-02,  2.4182e-02, -4.6656e-02],\n        [-2.9361e-02, -1.0741e-01,  2.9258e-02, -9.0035e-03, -3.6606e-02,\n         -8.4091e-02,  7.6264e-02,  3.0608e-02,  1.2083e-02, -3.5776e-02],\n        [ 3.7033e-03, -5.3160e-02, -6.2598e-02,  2.2187e-02, -4.1269e-02,\n         -1.5061e-01,  1.0993e-01, -1.8619e-01,  5.1323e-02, -3.9353e-02],\n        [ 8.2567e-02, -4.5244e-03, -8.3797e-02,  1.0872e-01, -4.6748e-02,\n          6.8438e-03, -6.3787e-02,  7.6138e-02, -5.7521e-02, -1.2464e-02],\n        [-4.0384e-02,  1.0084e-02,  4.9962e-02, -3.1437e-02,  5.7103e-02,\n         -8.4461e-02,  5.8014e-02,  3.8766e-02, -9.1711e-04,  3.7653e-02],\n        [-4.3991e-02, -7.7074e-02,  4.5000e-02,  1.9097e-02, -1.4656e-02,\n          4.5675e-04, -3.3925e-02,  1.2784e-01, -2.2476e-01, -3.9532e-02],\n        [ 5.6871e-02, -2.7520e-02, -3.2697e-02, -1.6940e-01,  1.3238e-01,\n          6.5076e-02,  1.8154e-02,  4.5399e-02,  5.4576e-02,  7.3361e-03],\n        [-1.9565e-02,  3.1256e-02,  1.1537e-03,  1.4080e-01,  3.7486e-03,\n          7.0257e-02, -1.1690e-01,  1.2542e-01,  8.9521e-02, -2.5217e-02],\n        [ 1.0400e-01,  3.6877e-02,  1.8879e-02,  4.7785e-02, -3.0034e-02,\n          1.2424e-01, -1.4202e-01, -3.6302e-02,  9.9296e-03,  8.2290e-03],\n        [-3.3458e-02, -5.3612e-02, -4.4275e-02, -2.2179e-02,  1.1311e-03,\n          7.2850e-02, -4.5334e-02,  4.3296e-02,  2.5688e-02, -4.5821e-02],\n        [-3.3316e-02,  8.4964e-03, -6.1911e-02, -2.6158e-03, -3.0575e-02,\n          1.5768e-03,  9.7332e-02, -3.6887e-02,  9.6693e-02,  3.0724e-02],\n        [ 1.6858e-02,  2.4339e-03,  1.2680e-01, -6.1130e-02,  3.7795e-02,\n          1.5958e-01,  3.9345e-02,  7.5929e-03,  4.1675e-02,  3.9504e-02],\n        [-2.0064e-02,  8.0775e-02, -2.9338e-02,  2.5672e-02,  1.0890e-01,\n          7.5292e-02, -8.3407e-02,  2.4499e-02, -3.6358e-02, -2.7091e-02],\n        [ 2.6714e-02, -1.1057e-02, -1.1991e-01,  2.2168e-02, -2.2002e-02,\n          1.4614e-01, -2.8918e-02, -4.3205e-02, -9.6220e-03,  7.8855e-02],\n        [ 6.8199e-02, -3.1008e-02,  1.2264e-01, -1.4938e-02, -2.1505e-02,\n         -7.6834e-03, -1.8485e-02, -2.2138e-02, -5.0308e-02, -5.7137e-03],\n        [ 3.9741e-02, -8.4679e-02,  1.2151e-02, -6.9012e-03, -5.5282e-02,\n          7.5844e-02, -1.0451e-01, -1.7195e-01,  1.1883e-02, -8.6847e-02],\n        [-1.9688e-01,  4.8951e-02, -9.4198e-02,  7.8181e-02, -9.8828e-03,\n          9.2158e-02,  4.4263e-03,  9.6399e-02, -2.3000e-02,  3.7946e-02],\n        [-1.6565e-02,  8.3632e-02, -3.1503e-02, -2.0136e-02,  3.0845e-02,\n          1.2288e-01, -7.5772e-02, -4.0255e-02, -2.5306e-02,  1.7169e-03],\n        [ 6.7174e-02, -7.9315e-02, -1.4049e-03,  6.6502e-02, -6.5091e-02,\n          1.2987e-02, -1.0155e-01,  7.4147e-02, -4.8684e-02, -7.6313e-02],\n        [-6.3926e-02,  8.7817e-02, -1.3655e-02,  5.6434e-03,  4.8332e-02,\n          3.8771e-02, -9.4088e-03,  6.6904e-02, -1.1947e-01,  1.2877e-02],\n        [ 5.7764e-02,  3.2096e-02,  2.7364e-02, -6.5782e-02,  1.5690e-02,\n          6.0123e-02,  5.3388e-02, -1.1635e-02,  6.7354e-02,  7.3766e-02],\n        [-9.5869e-03,  8.7751e-02,  6.5057e-02,  3.7782e-02, -2.8177e-02,\n          3.5619e-02, -7.5904e-02,  1.9153e-01,  9.2821e-02, -4.0765e-02],\n        [-1.4732e-01,  1.2855e-01, -2.9558e-02, -3.6918e-02, -9.2483e-02,\n          6.4201e-02,  7.8915e-02, -3.9987e-02,  6.0675e-02, -7.1693e-03],\n        [ 4.4523e-02,  1.4501e-02,  5.2027e-02,  6.7996e-03,  1.8010e-02,\n         -1.7958e-02, -4.0902e-02,  3.8452e-03, -1.2803e-01, -7.6590e-02],\n        [ 4.1396e-02, -5.6186e-02, -1.6062e-03, -3.0542e-02, -4.8735e-02,\n         -7.7626e-02,  3.8983e-02, -3.3774e-02,  1.3775e-02, -1.3410e-02],\n        [ 6.3276e-02, -1.6514e-01,  5.1903e-02, -1.5183e-02, -9.7894e-02,\n         -3.9018e-02, -3.0149e-02,  8.7333e-03,  7.9724e-02,  5.1520e-02],\n        [ 4.6044e-02, -3.4314e-02,  5.1686e-02, -7.9949e-02, -3.2526e-02,\n         -2.6578e-02,  5.2153e-02,  1.4759e-01, -4.2293e-02,  7.7393e-02],\n        [-3.6437e-02,  5.5856e-02,  8.4661e-02, -3.0616e-02,  1.0673e-03,\n          2.2598e-02,  9.6911e-02, -1.3725e-02, -9.2214e-02, -4.9727e-02],\n        [ 9.1172e-02, -7.9199e-02, -2.7897e-02,  6.2270e-02, -8.4980e-02,\n         -2.4153e-02, -1.3289e-01, -7.0886e-02, -3.0780e-02,  8.4179e-02],\n        [-7.9551e-02,  2.5706e-02, -1.7994e-02, -6.9913e-03,  1.0451e-01,\n         -4.7109e-02, -4.1839e-02, -1.0196e-02,  5.6815e-02, -1.1829e-01],\n        [-1.1783e-02,  7.6400e-02,  7.1504e-02, -1.3724e-01, -2.9943e-02,\n          1.2327e-02,  7.5568e-02,  7.2082e-03,  3.7335e-03,  4.1607e-03],\n        [-1.5383e-02,  7.8986e-02,  2.8162e-02,  3.3195e-02,  9.4126e-02,\n         -4.7889e-02, -1.8477e-02,  8.1244e-02,  1.1377e-02,  1.5283e-01],\n        [-5.0483e-02,  6.4596e-02,  6.2384e-02, -7.5074e-02,  7.2435e-02,\n          2.0802e-02,  4.0618e-03, -1.6385e-02,  6.4054e-02, -2.7671e-02],\n        [ 1.4134e-01,  4.0917e-03,  3.7652e-02,  6.7660e-02, -9.1541e-04,\n          8.4696e-02,  6.0580e-02, -1.8102e-02, -7.8428e-02, -8.7941e-02],\n        [-6.7472e-02, -8.8073e-02, -7.4903e-02,  8.7533e-02, -1.3176e-02,\n          1.4801e-01,  7.5961e-03,  1.6319e-02, -6.4150e-02,  2.0230e-02],\n        [ 5.7140e-02, -6.4432e-03,  4.8545e-02, -6.6390e-02,  2.1310e-02,\n         -4.9351e-02, -2.0682e-02, -1.2687e-01, -1.4224e-01, -1.1621e-01],\n        [-2.1375e-02, -1.4452e-02, -1.0473e-01, -5.8254e-02,  1.7305e-01,\n          5.4051e-03, -1.9611e-02,  4.8505e-02, -1.1192e-01, -4.6737e-02],\n        [ 6.6150e-02,  1.6583e-01,  2.5004e-03,  8.9390e-02, -4.5832e-02,\n         -6.4372e-02, -1.1386e-01, -9.6375e-03,  1.5853e-02, -6.5092e-02],\n        [ 3.8597e-02,  1.0170e-01,  4.6857e-02,  1.2955e-01,  3.4016e-02,\n          7.8140e-02, -2.1197e-02, -6.7638e-02, -1.2268e-01, -7.4382e-02],\n        [-7.0604e-02,  5.7156e-02, -4.3319e-02,  4.8755e-02, -4.7680e-03,\n          1.2751e-02,  5.3069e-02, -1.2289e-02, -9.4971e-02,  5.4835e-02],\n        [-3.7956e-02, -3.8104e-02, -1.0841e-01,  1.3253e-01, -7.7401e-02,\n         -3.4238e-02,  1.2895e-01, -6.7334e-03,  5.6274e-03, -3.6142e-02],\n        [-3.5497e-02,  8.6382e-03, -4.5329e-02, -8.4018e-02,  6.6362e-02,\n         -2.6559e-02, -8.4784e-02,  5.6983e-02,  2.2961e-02, -7.8149e-03],\n        [-3.2115e-02,  6.3245e-02, -1.0399e-02, -3.9937e-02, -3.8008e-02,\n          1.7057e-01,  8.4447e-03,  9.2705e-03,  8.5171e-02,  3.8838e-02],\n        [-9.0666e-02, -5.7140e-03, -1.5410e-02,  2.3831e-02, -7.6910e-02,\n          1.6359e-02, -2.0926e-02,  6.4342e-02, -1.1237e-02,  1.1565e-02],\n        [-7.3061e-02, -7.7387e-02,  1.2551e-02, -2.3853e-02,  7.2864e-02,\n         -3.1551e-02, -4.2459e-02, -2.9551e-02,  4.5417e-02, -1.2046e-01],\n        [ 2.8418e-02,  2.9734e-02, -9.1364e-02, -6.1068e-02, -2.3202e-02,\n          5.1248e-02,  1.8532e-01, -1.8126e-02,  7.5860e-02,  2.4556e-03],\n        [-2.3522e-02, -3.0125e-02,  8.1039e-02,  9.6651e-03, -5.0949e-02,\n         -5.5351e-02, -3.1142e-02, -6.8061e-02,  5.6162e-02,  7.4335e-02],\n        [-1.7472e-02, -2.3037e-02,  3.3674e-02,  3.4075e-02,  6.3622e-02,\n         -4.9574e-02,  6.5163e-02,  8.9478e-03, -7.0402e-02,  2.7236e-05],\n        [ 3.2429e-02, -6.3728e-02,  2.4412e-02, -2.3002e-02,  4.1015e-02,\n         -5.7363e-03, -1.5803e-01, -2.2515e-03,  1.5487e-02,  5.1615e-03],\n        [-9.3287e-02,  1.4722e-02, -1.6753e-02,  6.2545e-03, -2.1398e-02,\n         -4.9796e-02, -9.2211e-02, -4.1343e-02, -2.2851e-03,  1.3606e-02],\n        [-1.6094e-02,  1.0595e-01, -1.9957e-02, -1.0196e-02,  3.8459e-02,\n         -5.4906e-03, -6.1404e-02,  3.2463e-02, -6.5779e-02, -3.5616e-02],\n        [ 4.2256e-02, -1.3215e-02,  8.4260e-03, -5.7226e-02, -5.9605e-03,\n          2.2233e-02, -8.4455e-02, -6.4578e-02, -3.9506e-02, -2.2221e-02],\n        [ 2.9438e-02, -3.5528e-02,  2.7829e-02,  1.6406e-02, -4.9294e-02,\n          9.9992e-02, -7.4397e-02,  1.7730e-01, -5.4076e-03, -4.3610e-02],\n        [-2.1693e-02, -1.5488e-01, -4.8218e-03, -1.2173e-01, -1.0582e-01,\n          1.6034e-02,  8.9133e-02,  2.7070e-02,  2.9875e-02, -4.1423e-02],\n        [ 1.7653e-02,  2.0178e-02, -6.1573e-02, -7.2830e-03,  1.2253e-01,\n         -7.0923e-02, -8.1860e-02,  1.2367e-01, -1.9879e-02,  1.0576e-01],\n        [-1.4109e-01, -1.0384e-02, -9.0179e-03, -2.5273e-02, -1.1766e-02,\n          3.5018e-02,  4.5524e-02,  4.5809e-02, -4.6353e-02, -8.6029e-02],\n        [ 3.3638e-02, -3.3754e-02,  3.3635e-02, -8.4041e-02, -3.0967e-02,\n         -9.1113e-02, -6.9617e-02,  7.9032e-02,  1.4268e-01,  1.1331e-01],\n        [-8.3792e-02,  3.7373e-02, -4.0281e-02, -4.9835e-02,  3.7364e-02,\n         -3.7161e-02,  8.6804e-02,  8.6280e-02, -1.2284e-01,  1.9238e-01],\n        [-9.8051e-02, -8.5927e-02,  6.1685e-02,  4.8515e-02,  1.4577e-01,\n          4.2600e-02, -6.8777e-02,  3.3320e-02,  1.8494e-03,  1.2764e-01],\n        [-8.1257e-02,  2.4775e-02, -4.6640e-02,  2.5767e-02, -7.6183e-02,\n          1.9111e-02,  6.4628e-02,  3.9491e-02, -1.1363e-01, -1.2222e-01],\n        [ 8.1916e-02,  5.1175e-02,  7.1828e-02,  4.3786e-02,  2.4209e-02,\n          1.3094e-01,  1.2266e-01,  6.7226e-02, -5.3989e-02,  1.2282e-01],\n        [ 1.0016e-01,  4.2827e-02, -1.1413e-04, -5.8562e-02,  9.2478e-02,\n         -9.3654e-03, -1.8664e-02,  6.1127e-02, -2.8290e-02,  1.1094e-02],\n        [-3.1247e-02, -1.3690e-02, -5.5080e-02,  4.1083e-02, -1.0141e-01,\n         -1.2531e-01,  5.9634e-02,  2.1113e-02,  7.7175e-02, -5.6688e-02],\n        [ 2.4041e-02,  4.6077e-02,  3.0765e-02, -8.0991e-03, -2.1220e-02,\n          4.9246e-02, -2.7895e-02, -7.1778e-04, -4.6787e-02, -1.9347e-03],\n        [ 1.3822e-03, -8.2286e-02,  8.3038e-02, -7.3320e-02, -4.4233e-02,\n         -1.3948e-01, -8.2189e-02,  1.2609e-01, -1.0012e-01, -9.4920e-02],\n        [ 6.9233e-02, -5.1392e-02, -1.1317e-01, -2.6815e-03,  2.2573e-01,\n         -1.3118e-01,  4.1881e-02, -3.5965e-02,  6.8770e-02, -7.3098e-02],\n        [ 8.6601e-02,  2.4762e-02, -2.7002e-02, -4.7169e-03, -1.1511e-02,\n         -6.0241e-02,  7.6464e-02, -2.1966e-02,  3.5967e-02,  6.8424e-02],\n        [-1.1660e-02, -3.4604e-03,  3.3204e-02, -1.1349e-02, -1.1317e-01,\n         -2.7086e-03, -7.0159e-02, -3.6693e-02,  4.9915e-02, -3.2986e-03],\n        [ 7.8933e-02, -5.4298e-02, -6.1570e-02,  2.8909e-02, -8.0871e-02,\n          1.5321e-01, -6.4480e-02, -8.4388e-02,  2.0626e-02, -8.0225e-03],\n        [ 1.0315e-01,  8.2326e-02, -6.0939e-02, -5.7266e-02,  1.4004e-02,\n         -7.6632e-02,  8.4691e-02,  7.7489e-02, -8.9126e-02,  7.4295e-02],\n        [-6.8323e-02,  7.2190e-02, -5.8388e-02, -1.2475e-01,  4.8006e-03,\n          9.9945e-02, -4.7132e-02,  1.2566e-01,  1.0964e-01, -3.8934e-02],\n        [-9.2739e-02,  4.5412e-02, -6.1549e-03, -1.6381e-01,  1.3795e-01,\n          1.6003e-02,  5.3267e-02,  1.4909e-02, -1.2181e-01, -4.5104e-02],\n        [-8.9216e-02, -5.7348e-02, -3.5239e-02, -7.6102e-02, -3.3625e-03,\n          5.7921e-02,  3.7301e-02, -6.5469e-02, -5.3079e-02,  1.7633e-02],\n        [-1.6496e-01,  4.0457e-03,  7.2553e-03,  4.1516e-02, -5.0928e-02,\n          6.3508e-03,  5.2355e-02,  5.4423e-02,  7.4410e-02,  1.7749e-02],\n        [ 1.2331e-01, -4.1607e-02,  4.9485e-02,  3.7912e-02, -5.7972e-03,\n         -3.6210e-02, -1.9738e-02,  9.8202e-02,  8.8004e-02, -6.1111e-02],\n        [ 6.8614e-02,  8.4619e-02, -6.6824e-02,  5.8337e-02,  5.4812e-03,\n          7.2324e-02,  3.8018e-02, -1.0440e-02, -1.6361e-02,  3.3607e-02],\n        [ 1.4162e-02, -1.0815e-02,  3.1555e-02,  8.2853e-02, -6.7328e-02,\n          5.3545e-02, -1.5559e-01,  5.7636e-03,  4.7684e-03, -6.3206e-03],\n        [ 5.4732e-02,  3.6263e-02,  1.1869e-02, -2.3069e-02, -5.1914e-02,\n         -2.2124e-02, -4.9120e-02, -1.1647e-02, -7.7245e-02, -1.0494e-01],\n        [ 1.9750e-02,  1.6213e-01, -4.7349e-02,  1.1983e-01, -1.9699e-01,\n          1.5567e-01,  2.1169e-02, -7.5804e-02, -3.9002e-02, -1.8460e-03],\n        [-1.0168e-02, -9.8894e-02,  1.4165e-01, -6.2231e-02, -9.5349e-02,\n          1.0664e-01, -1.3447e-01, -1.1643e-01, -3.5480e-02,  4.4460e-02],\n        [-1.3242e-02,  7.4441e-02,  1.4009e-01, -4.6272e-02, -4.5334e-02,\n          1.3162e-01,  8.4321e-02, -1.1435e-01,  2.8940e-03, -3.9990e-02],\n        [ 1.1540e-01, -2.3382e-02, -1.4272e-02, -1.4106e-03,  6.4413e-02,\n          5.6253e-02,  7.4876e-02, -3.4388e-02,  9.8249e-02, -7.8745e-02],\n        [ 3.4247e-03, -8.5144e-03, -6.0406e-03,  2.6354e-02,  2.4624e-02,\n          2.0294e-02,  4.7966e-03, -9.2052e-02, -8.9063e-02, -1.2629e-02],\n        [-1.1749e-01, -1.0309e-01, -1.2069e-01, -2.5819e-02, -4.8613e-02,\n         -1.6731e-02, -6.9904e-02, -1.7725e-02, -1.0027e-02, -6.8444e-02],\n        [ 3.3283e-02,  1.5316e-02,  1.5156e-01,  1.2894e-01, -2.8212e-02,\n          2.0965e-01,  9.3999e-02, -7.4824e-02,  3.5266e-04, -9.0989e-02],\n        [-1.0796e-01,  1.2651e-01, -2.1478e-02, -7.1910e-02, -9.3482e-02,\n          6.9413e-02, -6.1439e-02,  2.2308e-02,  3.0165e-02, -7.7031e-02],\n        [-2.5684e-02,  6.5865e-02,  7.2613e-02, -5.0703e-02, -8.2840e-02,\n         -2.0097e-02, -6.8013e-02,  1.5453e-02, -4.6401e-02, -2.8894e-02],\n        [-4.4468e-02,  1.4204e-02,  2.1952e-02,  1.5594e-02,  4.2291e-02,\n         -5.2361e-02,  7.0824e-02,  5.8891e-02, -1.6151e-02,  7.6689e-02]],\n       device='cuda:0', requires_grad=True)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization of the network\n",
    "weight_scale = 7*(1.0-beta)\n",
    "w1 = torch.empty((num_inputs, hidden_1_neurons), device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.empty((hidden_1_neurons, hidden_2_neurons), device=device, dtype=dtype, requires_grad=True)\n",
    "w3 = torch.empty((hidden_2_neurons, num_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale / np.sqrt(num_inputs))\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale / np.sqrt(hidden_1_neurons))\n",
    "torch.nn.init.normal_(w3, mean=0.0, std=weight_scale / np.sqrt(hidden_2_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def run_snn(inputs):\n",
    "    h1 = torch.einsum('abc,cd->abd', (inputs, w1))\n",
    "    syn_hidden_1 = torch.zeros((batch_size, hidden_1_neurons), device=device, dtype=dtype)\n",
    "    mem_hidden_1 = torch.zeros((batch_size, hidden_1_neurons), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec_hidden_1 = [mem_hidden_1]\n",
    "    spike_rec_hidden_1 = [mem_hidden_1]\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        mem_threshold = mem_hidden_1 - 1.0\n",
    "        out = spike_fn(mem_threshold)\n",
    "        rst = torch.zeros_like(mem_hidden_1)\n",
    "        c = (mem_threshold > 0)\n",
    "        rst[c] = torch.ones_like(mem_hidden_1)[c]\n",
    "\n",
    "        new_syn = alpha * syn_hidden_1 + h1[:, t]\n",
    "        new_mem = beta * mem_hidden_1 + syn_hidden_1 - rst\n",
    "        mem_hidden_1 = new_mem\n",
    "        syn_hidden_1 = new_syn\n",
    "\n",
    "        mem_rec_hidden_1.append(mem_hidden_1)\n",
    "        spike_rec_hidden_1.append(out)\n",
    "\n",
    "    spike_rec_hidden_1 = torch.stack(spike_rec_hidden_1, dim=1)\n",
    "    mem_rec_hidden_1 = torch.stack(mem_rec_hidden_1, dim=1)\n",
    "\n",
    "    h2 = torch.einsum('abc,cd->abd', (spike_rec_hidden_1, w2))\n",
    "    syn_hidden_2 = torch.zeros((batch_size, hidden_2_neurons), device=device, dtype=dtype)\n",
    "    mem_hidden_2 = torch.zeros((batch_size, hidden_2_neurons), device=device, dtype=dtype)\n",
    "\n",
    "    mem_2_rec = [mem_hidden_2]\n",
    "    spike_2_rec = [mem_hidden_2]\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        mem_threshold = mem_hidden_2 - 1.0\n",
    "        out = spike_fn(mem_threshold)\n",
    "        rst = torch.zeros_like(mem_hidden_2)\n",
    "        c = (mem_threshold > 0)\n",
    "        rst[c] = torch.ones_like(mem_hidden_2)[c]\n",
    "\n",
    "        new_syn = alpha * syn_hidden_2 + h2[:, t]\n",
    "        new_mem = beta * mem_hidden_2 + syn_hidden_2 - rst\n",
    "        mem_hidden_2 = new_mem\n",
    "        syn_hidden_2 = new_syn\n",
    "\n",
    "        mem_2_rec.append(mem_hidden_2)\n",
    "        spike_2_rec.append(out)\n",
    "\n",
    "    mem_2_rec = torch.stack(mem_2_rec, dim=1)\n",
    "    spike_2_rec = torch.stack(spike_2_rec, dim=1)\n",
    "\n",
    "    h3 = torch.einsum('abc,cd->abd', (spike_2_rec, w3))\n",
    "    flt = torch.zeros((batch_size, num_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size, num_outputs), device=device, dtype=dtype)\n",
    "\n",
    "    out_rec = [out]\n",
    "    for t in range(num_steps):\n",
    "        new_flt = alpha * flt + h3[:, t]\n",
    "        new_out = beta * out + flt\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "        out_rec.append(out)\n",
    "    \n",
    "    out_rec = torch.stack(out_rec, dim=1)\n",
    "    prev_recs = [[mem_rec_hidden_1, spike_rec_hidden_1], [mem_2_rec, spike_2_rec]]\n",
    "\n",
    "    return out_rec, prev_recs\n",
    "\n",
    "def train(x_data, y_data, lr=1e-3, nb_epochs=10):\n",
    "    params = [w1, w2, w3]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    loss_hist = []\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, num_inputs):\n",
    "            output, _ = run_snn(x_local.to_dense())\n",
    "            m, _ = torch.max(output, 1)\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "            loss_val = loss_fn(log_p_y, y_local)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        print(\"Epoch %i: loss=%.5f\"%(epoch+1,mean_loss))\n",
    "        loss_hist.append(mean_loss)\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "def compute_classification_accuracy(x_data, y_data):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, num_steps, num_inputs, shuffle=False):\n",
    "        output,_ = run_snn(x_local.to_dense())\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.77786\n",
      "Epoch 2: loss=0.22627\n",
      "Epoch 3: loss=0.16952\n",
      "Epoch 4: loss=0.13326\n",
      "Epoch 5: loss=0.11189\n",
      "Epoch 6: loss=0.09399\n",
      "Epoch 7: loss=0.07980\n",
      "Epoch 8: loss=0.07071\n",
      "Epoch 9: loss=0.06355\n",
      "Epoch 10: loss=0.05505\n",
      "Epoch 11: loss=0.04908\n",
      "Epoch 12: loss=0.04265\n",
      "Epoch 13: loss=0.03830\n",
      "Epoch 14: loss=0.03474\n",
      "Epoch 15: loss=0.02910\n",
      "Epoch 16: loss=0.02740\n",
      "Epoch 17: loss=0.02437\n",
      "Epoch 18: loss=0.02353\n",
      "Epoch 19: loss=0.02099\n",
      "Epoch 20: loss=0.01873\n",
      "Epoch 21: loss=0.01761\n",
      "Epoch 22: loss=0.01705\n",
      "Epoch 23: loss=0.01435\n",
      "Epoch 24: loss=0.01446\n",
      "Epoch 25: loss=0.01188\n",
      "Epoch 26: loss=0.01175\n",
      "Epoch 27: loss=0.01131\n",
      "Epoch 28: loss=0.00942\n",
      "Epoch 29: loss=0.00715\n",
      "Epoch 30: loss=0.00922\n",
      "Training accuracy: 0.996\n",
      "Test accuracy: 0.976\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 495x300 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loss_hist = train(x_train, y_train, lr=2e-4, nb_epochs=30)\n",
    "plt.figure(figsize=(3.3,2),dpi=150)\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "sns.despine()\n",
    "\n",
    "print(\"Training accuracy: %.3f\"%(compute_classification_accuracy(x_train,y_train)))\n",
    "print(\"Test accuracy: %.3f\"%(compute_classification_accuracy(x_test,y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9c9070f4",
   "language": "python",
   "display_name": "PyCharm (use-of-snn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}