{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook contains code to run the P300 Multi-subject EEG Guess The Number Dataset classification using a spiking neural\n",
    "network.\n",
    "\n",
    "To run the notebook, please download the dataset here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G9RRLN\n",
    "and place it in the **dataset** folder (do not rename it unless you change the name of the file in the notebook as well)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import os\n",
    "import nengo\n",
    "import keras\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from tensorflow.python.keras import Input, Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import Conv2D, BatchNormalization, Dropout, AveragePooling2D, Flatten, Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First load the dataset and set seeds for consistency\n",
    "# The dataset should be by default saved in ../datasets/VarekaGTNEpochs.mat\n",
    "# The file can be downloaded here https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G9RRLN\n",
    "dataset_path = os.path.join('dataset', 'VarekaGTNEpochs.mat')\n",
    "\n",
    "params_folder_path = 'nengo_network_params' # path to saved parameters\n",
    "os.makedirs('nengo_network_params', exist_ok=True)\n",
    "\n",
    "seed = 0 # constant seed\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Features shape: (8036, 1, 3600), Labels shape: (8036, 1, 2)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = loadmat(dataset_path)\n",
    "target_data, non_target_data = mat['allTargetData'], mat['allNonTargetData'] # get target and non-target data\n",
    "features = np.concatenate((target_data, non_target_data))\n",
    "\n",
    "# Target labels are represented as (1, 0) vector, non target labels are represented as (0, 1) vector\n",
    "target_labels = np.tile(np.array([1, 0]), (target_data.shape[0], 1)) # set 'target' as (1, 0) vector\n",
    "non_target_labels = np.tile(np.array([0, 1]), (non_target_data.shape[0], 1)) # set 'non target' as (0, 1) vector\n",
    "labels = np.vstack((target_labels, non_target_labels)) # concatenate target and non target labels\n",
    "\n",
    "# Filter noise above 100 uV\n",
    "threshold = 100.0\n",
    "x_result, y_result = [], []\n",
    "for i in range(features.shape[0]):\n",
    "    if not np.max(np.abs(features[i])) > threshold:\n",
    "        x_result.append(features[i])\n",
    "        y_result.append(labels[i])\n",
    "\n",
    "# Save data to numpy array\n",
    "features, labels = np.array(x_result), np.array(y_result)\n",
    "features = features.reshape((features.shape[0], 1, -1))\n",
    "labels = labels.reshape((labels.shape[0], 1, -1))\n",
    "\n",
    "# Print shapes\n",
    "f'Features shape: {features.shape}, Labels shape: {labels.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to create the CNN model\n",
    "# Slightly modified version from https://www.sciencedirect.com/science/article/pii/S1746809419304185\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Function to create tensorflow model\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(3, 1200, 1), name='input_layer')\n",
    "    conv2d = Conv2D(filters=6, kernel_size=(3, 3), activation=tf.nn.relu)(inp)\n",
    "    dropout1 = Dropout(0.5, seed=seed)(conv2d)\n",
    "    avg_pooling = AveragePooling2D(pool_size=(1, 8), padding='same')(dropout1)\n",
    "    flatten = Flatten()(avg_pooling)\n",
    "    dense1 = Dense(100, activation=tf.nn.relu)(flatten)\n",
    "    batch_norm = BatchNormalization()(dense1)\n",
    "    dropout2 = Dropout(0.5, seed=seed)(batch_norm)\n",
    "    output = Dense(2, activation=tf.nn.softmax, name='output_layer')(dropout2)\n",
    "\n",
    "    return Model(inputs=inp, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Since we used [1,0] as true (target) we also need to reference the correct index during metrics evaluation.\n",
    "# In sklearn this means that we set the pos_label to 0 instead of 1\n",
    "true_ref_idx = np.argmax(np.array([1, 0]))\n",
    "\n",
    "def get_metrics(simulator, output_layer, x_test, y_test, minibatch_size, network_name):\n",
    "    \"\"\"\n",
    "    Function for calculating metrics\n",
    "    :param simulator: simulator instance\n",
    "    :param input_layer: input layer reference\n",
    "    :param output_layer: output layer reference\n",
    "    :param x_test: features of the testing subset\n",
    "    :param y_test: labels of the testing subset\n",
    "    :param network_name: name of the network\n",
    "    :return: accuracy, recall and precision metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate the remaining number of samples since the predict function uses minibatch_size as well (i.e.\n",
    "    samples = (x_test.shape[0] // minibatch_size ) * minibatch_size\n",
    "    x_test, y_test = x_test[:samples], y_test[:samples]\n",
    "\n",
    "    predictions = simulator.predict(x_test)[output_layer] # get result from output layer when predicting on x_test\n",
    "    predictions = predictions[:,-1,:] # get the last timestep\n",
    "    predictions_argm = np.argmax(predictions, axis=-1) # get predicted label\n",
    "\n",
    "    y_test = np.squeeze(y_test, axis=1) # remove time dimension\n",
    "    y_test_argm = np.argmax(y_test, axis=-1) # get labels, due to one-hot encoding 0 = target, 1 = non-target\n",
    "\n",
    "    precision = metrics.precision_score(y_true=y_test_argm, y_pred=predictions_argm, pos_label=true_ref_idx) # get precision score\n",
    "    recall = metrics.recall_score(y_true=y_test_argm, y_pred=predictions_argm, pos_label=true_ref_idx) # get recall\n",
    "    f1 = metrics.f1_score(y_true=y_test_argm, y_pred=predictions_argm, pos_label=true_ref_idx)\n",
    "    accuracy = metrics.accuracy_score(y_true=y_test_argm, y_pred=predictions_argm) # get accuracy\n",
    "\n",
    "    # First column - \"0\" = target samples, second column \"1\" = non-target samples\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true=y_test_argm, y_pred=predictions_argm)\n",
    "\n",
    "    # Log the statistics\n",
    "    print(f'{network_name}: accuracy = {accuracy * 100}%, precision = {precision}, '\n",
    "          f'recall = {recall}, f1 = {f1}')\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    return accuracy, precision, recall, f1, confusion_matrix\n",
    "\n",
    "# Define functions to run the analog and spiking networks\n",
    "\n",
    "def run_ann(model, train, valid, test, params_save_path, iteration, shuffle_training=True):\n",
    "    \"\"\"\n",
    "    Run ann via Nengo simulator. This fits the given model with the training data (train) and validates it using validation\n",
    "    data (valid). Then accuracy is calculated using the test data (test) and weights are saved to params_save_path\n",
    "    :param shuffle_training: whether to shuffle data (default true)\n",
    "    :param model: tensorflow model created from create_model() function\n",
    "    :param train: pair of features and labels from training data\n",
    "    :param valid: pair of features and labels from validation data\n",
    "    :param test: pair of features and labels from test data\n",
    "    :param params_save_path: output path to save weights of the network for SNN testing\n",
    "    :return accuracy, precision, recall, f1 and confusion matrix from the testing data\n",
    "    \"\"\"\n",
    "\n",
    "    # unwrap into training and testing data for each subset\n",
    "    x_train, y_train = train[0], train[1]\n",
    "    x_valid, y_valid = valid[0], valid[1]\n",
    "    x_test, y_test = test[0], test[1]\n",
    "\n",
    "    converter = nengo_dl.Converter(model)\n",
    "    with nengo_dl.Simulator(converter.net, minibatch_size=64) as simulator:\n",
    "        # Compile the model with binary cross-entropy and Adam optimizer\n",
    "        simulator.compile(\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            loss=keras.losses.BinaryCrossentropy(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        input_layer = converter.inputs[model.get_layer('input_layer')] # get nengo input layer\n",
    "        output_layer = converter.outputs[model.get_layer('output_layer')] # get nengo output layer\n",
    "\n",
    "        simulator.fit(\n",
    "            x={ input_layer: x_train }, y={ output_layer: y_train },\n",
    "            validation_data=({ input_layer: x_valid }, { output_layer: y_valid }),\n",
    "            epochs=30,\n",
    "            shuffle=shuffle_training,\n",
    "            callbacks=[EarlyStopping(patience=5, verbose=1, restore_best_weights=True)] # early stop to avoid overfitting\n",
    "        ) # train model\n",
    "\n",
    "        simulator.save_params(params_save_path) # save params for SNN\n",
    "\n",
    "        accuracy, precision, recall, f1, confusion_matrix = get_metrics(simulator, output_layer, x_test, y_test,\n",
    "                                                  minibatch_size=simulator.minibatch_size,\n",
    "                                                  network_name=f'{iteration}. ANN')\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': confusion_matrix\n",
    "        }\n",
    "\n",
    "\n",
    "def run_snn(model, test, params_load_path, timesteps, scale_firing_rates, synapse, iteration):\n",
    "    \"\"\"\n",
    "    Runs SNN on test data. Loads pre-trained weights from params_load path and uses timesteps, scale_firing_rates and synapse\n",
    "    parameters for simulator.\n",
    "    :param model: reference to the tensorflow model\n",
    "    :param test: reference to the test features and labels\n",
    "    :param params_load_path: path to the saved weights of the ANN\n",
    "    :param timesteps: number of timesteps - i.e. how long is the input streamed to the network\n",
    "    :param scale_firing_rates: firing rate scaling - amplifies spikes\n",
    "    :param synapse: synaptic smoothing\n",
    "    :param iteration: iteration to print the result\n",
    "    :return: accuracy, precision, recall, f1 and confusion matrix from the testing data\n",
    "    \"\"\"\n",
    "\n",
    "    # Conversion of the TensorFlow model to a spiking Nengo model\n",
    "    converter = nengo_dl.Converter(\n",
    "        model=model,\n",
    "        swap_activations={ tf.nn.relu: nengo.SpikingRectifiedLinear() },\n",
    "        scale_firing_rates=scale_firing_rates,\n",
    "        synapse=synapse\n",
    "    )\n",
    "\n",
    "    x_test, y_test = test[0], test[1] # get test features and labels\n",
    "\n",
    "    with converter.net:\n",
    "        nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    output_layer = converter.outputs[model.get_layer('output_layer')] # output layer for simulator\n",
    "    x_test_time_tiled = np.tile(x_test, (1, timesteps, 1)) # tile x_test to match desired timesteps for simulator\n",
    "\n",
    "    with nengo_dl.Simulator(converter.net, minibatch_size=41, progress_bar=False) as simulator:\n",
    "        simulator.load_params(params_load_path)\n",
    "\n",
    "        # Name of the network for print in get_metrics function\n",
    "        name = f'{iteration}. SNN [timesteps={timesteps}, scale_firing_rates={scale_firing_rates}, synapse={synapse}]'\n",
    "        accuracy, precision, recall, f1, confusion_matrix = get_metrics(simulator, output_layer, x_test_time_tiled, y_test,\n",
    "                                                  minibatch_size=simulator.minibatch_size,\n",
    "                                                  network_name=name)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': confusion_matrix\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (6027, 1, 3600), train labels shape: (6027, 1, 2)\n",
      "Test features shape: (2009, 1, 3600), test labels shape: (2009, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=seed, shuffle=True)\n",
    "\n",
    "print(f'Train features shape: {x_train.shape}, train labels shape: {y_train.shape}')\n",
    "print(f'Test features shape: {x_test.shape}, test labels shape: {y_test.shape}')\n",
    "\n",
    "# Arrays to store results from the ANN and the SNN\n",
    "ann, snn = [], {}\n",
    "\n",
    "# Configuration for the spiking network with format: timesteps, scale_firing_rates, synapse\n",
    "snn_config = [\n",
    "    [50, 1000, 0.01], # best performing parameters for simulator\n",
    "    [50, 1000, None], # synaptic smoothing turned off\n",
    "    [50, 1, 0.01], # spike scaling turned off\n",
    "    [50, 1, None] # everything turned off, only RELU is swapped for spiking RELU\n",
    "]\n",
    "\n",
    "variants = []\n",
    "for variant in snn_config:\n",
    "        # name of the configuration\n",
    "        name = f'snn [timesteps={variant[0]}, scaling={variant[1]}, synapse=None]' if variant[2] is None \\\n",
    "            else 'snn [timesteps={}, scaling={}, synapse={:3f}]'.format(variant[0], int(variant[1]), variant[2])\n",
    "        variants.append(name)\n",
    "        snn[name] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/30\n",
      "Constructing graph: build stage finished in 0:00:00                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\converter.py:324: UserWarning: Layer type <class 'tensorflow.python.keras.layers.core.Dropout'> does not have a registered converter. Falling back to TensorNode.\n",
      "  warnings.warn(\n",
      "c:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\converter.py:324: UserWarning: average_pooling2d_1.padding has value same != valid, which is not supported. Falling back to TensorNode.\n",
      "  warnings.warn(\n",
      "c:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\converter.py:324: UserWarning: Cannot convert BatchNormalization layer to native Nengo objects unless inference_only=True or layer.trainable=False. Falling back to TensorNode.\n",
      "  warnings.warn(\n",
      "c:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\converter.py:586: UserWarning: Activation type <function softmax_v2 at 0x0000019BC5A16430> does not have a native Nengo equivalent; falling back to a TensorNode\n",
      "  warnings.warn(\n",
      "c:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\simulator.py:1927: UserWarning: Number of elements in input data (1507) is not evenly divisible by Simulator.minibatch_size (64); input data will be truncated.\n",
      "  warnings.warn(\n",
      "c:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\simulator.py:1927: UserWarning: Number of elements in input data (4520) is not evenly divisible by Simulator.minibatch_size (64); input data will be truncated.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-8cc080bcd628>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[1;31m# Run the analog network - train and evaluate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m      \u001B[1;31m# run ann\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m     ann_result = run_ann(model=model,\n\u001B[0m\u001B[0;32m     20\u001B[0m                          \u001B[0mtrain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train_curr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train_curr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m                          \u001B[0mvalid\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_val_curr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_val_curr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-12-897a62aed1ff>\u001B[0m in \u001B[0;36mrun_ann\u001B[1;34m(model, train, valid, test, params_save_path, iteration, shuffle_training)\u001B[0m\n\u001B[0;32m     74\u001B[0m         \u001B[0moutput_layer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconverter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_layer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'output_layer'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;31m# get nengo output layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 76\u001B[1;33m         simulator.fit(\n\u001B[0m\u001B[0;32m     77\u001B[0m             \u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m \u001B[0minput_layer\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx_train\u001B[0m \u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m \u001B[0moutput_layer\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0my_train\u001B[0m \u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m             \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m \u001B[0minput_layer\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx_valid\u001B[0m \u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m{\u001B[0m \u001B[0moutput_layer\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0my_valid\u001B[0m \u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo\\utils\\magic.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    179\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwrapped\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minstance\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    180\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 181\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minstance\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    182\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m             \u001B[0minstance\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"__self__\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\simulator.py\u001B[0m in \u001B[0;36mrequire_open\u001B[1;34m(wrapped, instance, args, kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m         )\n\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\simulator.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, n_steps, stateful, **kwargs)\u001B[0m\n\u001B[0;32m    866\u001B[0m                 \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"validation_data\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    867\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 868\u001B[1;33m         return self._call_keras(\n\u001B[0m\u001B[0;32m    869\u001B[0m             \u001B[1;34m\"fit\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_steps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn_steps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstateful\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstateful\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    870\u001B[0m         )\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo\\utils\\magic.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    179\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwrapped\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minstance\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    180\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 181\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minstance\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    182\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m             \u001B[0minstance\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"__self__\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\simulator.py\u001B[0m in \u001B[0;36mwith_self\u001B[1;34m(wrapped, instance, args, kwargs)\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minstance\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor_graph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 50\u001B[1;33m             \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     51\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackend\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_floatx\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeras_dtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\nengo_dl\\simulator.py\u001B[0m in \u001B[0;36m_call_keras\u001B[1;34m(self, func_type, x, y, n_steps, stateful, **kwargs)\u001B[0m\n\u001B[0;32m   1042\u001B[0m             \u001B[0mfunc_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1044\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1045\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1046\u001B[0m         \u001B[1;31m# update n_steps/time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1098\u001B[0m                 _r=1):\n\u001B[0;32m   1099\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1100\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1101\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1102\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 828\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"xla\"\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    886\u001B[0m         \u001B[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    887\u001B[0m         \u001B[1;31m# stateless function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 888\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    889\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    890\u001B[0m       \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfiltered_flat_args\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2939\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2940\u001B[0m       (graph_function,\n\u001B[1;32m-> 2941\u001B[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0m\u001B[0;32m   2942\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m   2943\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   3359\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3360\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3361\u001B[1;33m           \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3362\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3363\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   3194\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3195\u001B[0m     graph_function = ConcreteFunction(\n\u001B[1;32m-> 3196\u001B[1;33m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[0;32m   3197\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3198\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   1029\u001B[0m         if x is not None)\n\u001B[0;32m   1030\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1031\u001B[1;33m     \u001B[0mfunc_graph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvariables\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvariables\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1032\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1033\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0madd_control_dependencies\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001B[0m in \u001B[0;36m__exit__\u001B[1;34m(self, unused_type, unused_value, unused_traceback)\u001B[0m\n\u001B[0;32m    378\u001B[0m       \u001B[1;31m# consumed. This covers the case when the read value is returned from\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    379\u001B[0m       \u001B[1;31m# the function since that goes through a tf.identity in mark_as_return.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 380\u001B[1;33m       if (op_def_registry.get(op.type) is None or\n\u001B[0m\u001B[0;32m    381\u001B[0m           (op_is_stateful(op) and\n\u001B[0;32m    382\u001B[0m            (op.type not in utils.RESOURCE_READ_OPS or\n",
      "\u001B[1;32mc:\\dev\\anaconda\\envs\\spiking-nn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mtype\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2422\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2423\u001B[0m     \u001B[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2424\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mpywrap_tf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_OperationOpType\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_c_op\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2425\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2426\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 30 # 30 iterations of CV\n",
    "iteration = 1 # number of the current iteration\n",
    "val_size = 0.25 # 25% of the data is used as validation data\n",
    "for train_idx, val_idx in ShuffleSplit(n_splits=num_iterations, test_size=val_size, random_state=seed).split(x_train):\n",
    "    print(f'Iteration: {iteration}')\n",
    "\n",
    "    # Split all training data into current training data and validation data\n",
    "    x_train_curr, y_train_curr = x_train[train_idx], y_train[train_idx]\n",
    "    x_val_curr, y_val_curr = x_train[val_idx], y_train[val_idx]\n",
    "\n",
    "    # Set params path\n",
    "    params_path = os.path.join(params_folder_path, f'params_iter_{iteration}')\n",
    "\n",
    "    # Create an untrained model\n",
    "    model = create_model()\n",
    "\n",
    "    # Run the analog network - train and evaluate\n",
    "     # run ann\n",
    "    ann_result = run_ann(model=model,\n",
    "                         train=(x_train_curr, y_train_curr),\n",
    "                         valid=(x_val_curr, y_val_curr),\n",
    "                         test=(x_test, y_test),\n",
    "                         params_save_path=params_path,\n",
    "                         iteration=iteration\n",
    "                         )\n",
    "    K.clear_session() # clear session\n",
    "    ann.append(ann_result) # append the result to the ANN array\n",
    "\n",
    "    # Create an array where results for each configuration of the spiking network will be stored\n",
    "    snn_config_results = []\n",
    "    for i, variant_conf in enumerate(snn_config):\n",
    "        snn_result = run_snn(model=model,\n",
    "                          test=(x_test, y_test),\n",
    "                          params_load_path=params_path,\n",
    "                          timesteps=variant_conf[0],\n",
    "                          scale_firing_rates=variant_conf[1],\n",
    "                          synapse=variant_conf[2],\n",
    "                          iteration=iteration\n",
    "                          )\n",
    "        K.clear_session() # clear session\n",
    "        snn[variants[i]].append(snn_result) # save results\n",
    "\n",
    "    del model # delete the model (if this is not called it \"may\" create a memory leak - depends on the simulation machine)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create dictionary with the data for pandas dataframe\n",
    "data = {\n",
    "    'iterations': [x for x in range(1, num_iterations + 1)],\n",
    "    'ann_accuracy': [x['accuracy'] for x in ann],\n",
    "    'ann_precision': [x['precision'] for x in ann],\n",
    "    'ann_recall': [x['recall'] for x in ann],\n",
    "    'ann_f1': [x['f1'] for x in ann]\n",
    "}\n",
    "\n",
    "for variant_name in variants:\n",
    "    data[f'{variant_name}_accuracy'] = [x['accuracy'] for x in snn[variant_name]]\n",
    "    data[f'{variant_name}_precision'] = [x['precision'] for x in snn[variant_name]]\n",
    "    data[f'{variant_name}_recall'] = [x['recall'] for x in snn[variant_name]]\n",
    "    data[f'{variant_name}_f1'] = [x['f1'] for x in snn[variant_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create pandas dataframe and save it to xlsx file\n",
    "output_folder_path = 'p300_exp_output'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_excel(os.path.join(output_folder_path, 'data_iterations.xlsx'))\n",
    "\n",
    "'File with iteration data successfully saved.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network_names = ['ann'] + variants # names of each network in the pandas data frame\n",
    "data_stats = {\n",
    "    'models': network_names,\n",
    "    'average_acc': [],\n",
    "    'max_acc': [],\n",
    "    'std_acc': [],\n",
    "    'average_precision': [],\n",
    "    'max_precision': [],\n",
    "    'average_recall': [],\n",
    "    'max_recall': [],\n",
    "    'average_f1': [],\n",
    "    'max_f1': []\n",
    "} # statistics from the experiment\n",
    "\n",
    "average_acc, max_acc, std_acc = [], [], []\n",
    "average_precision, max_precision = [], []\n",
    "average_recall, max_recall = [], []\n",
    "for network_name in network_names:\n",
    "    data_stats['average_acc'].append(df[f'{network_name}_accuracy'].mean())\n",
    "    data_stats['max_acc'].append(df[f'{network_name}_accuracy'].max())\n",
    "    data_stats['std_acc'].append(df[f'{network_name}_accuracy'].std())\n",
    "\n",
    "    data_stats['average_precision'].append(df[f'{network_name}_precision'].mean())\n",
    "    data_stats['max_precision'].append(df[f'{network_name}_precision'].max())\n",
    "\n",
    "    data_stats['average_recall'].append(df[f'{network_name}_recall'].mean())\n",
    "    data_stats['max_recall'].append(df[f'{network_name}_recall'].max())\n",
    "\n",
    "    data_stats['average_f1'].append(df[f'{network_name}_f1'].mean())\n",
    "    data_stats['max_f1'].append(df[f'{network_name}_f1'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(data_stats)\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_stats.to_excel(os.path.join(output_folder_path, 'statistics.xlsx'))\n",
    "\n",
    "'File with statistics successfully saved.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}