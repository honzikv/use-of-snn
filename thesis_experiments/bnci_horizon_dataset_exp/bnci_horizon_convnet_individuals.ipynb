{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import nengo_dl\n",
    "from tensorflow.python.keras import Input, Model\n",
    "import nengo\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import Conv2D, Dropout, AveragePooling2D, Flatten, Dense, BatchNormalization, Conv3D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of files with samples from each participant\n",
    "dataset_path = os.path.join('dataset_result')\n",
    "files = [os.path.join(dataset_path, 'P{:02d}.npz'.format(i+1)) for i in range(18)] # P01 - P18 files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set seed to produce consistent result\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to create the CNN model\n",
    "def cnn_model():\n",
    "    inp = Input(shape=(14, 360, 1), name='input_layer')\n",
    "    conv1 = Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu,)(inp)\n",
    "    dropout1 = Dropout(0.2, seed=seed)(conv1)\n",
    "    avg_pool1 = AveragePooling2D(pool_size=(2, 2))(dropout1)\n",
    "    conv2 = Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu)(avg_pool1)\n",
    "    dropout2 = Dropout(0.2, seed=seed)(conv2)\n",
    "    avg_pool2 = AveragePooling2D(pool_size=(2, 2))(dropout2)\n",
    "    flatten = Flatten()(avg_pool2)\n",
    "    dense1 = Dense(512, activation=tf.nn.relu)(flatten)\n",
    "    dropout3 = Dropout(0.2, seed=seed)(dense1)\n",
    "    dense2 = Dense(256, activation=tf.nn.relu)(dropout3)\n",
    "    output = Dense(2, activation=tf.nn.softmax, name='output_layer')(dense2)\n",
    "\n",
    "    return Model(inputs=inp, outputs=output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_metrics(simulator, output_layer, x_test, y_test, minibatch_size, network_name):\n",
    "    \"\"\"\n",
    "    Function for calculating metrics\n",
    "    :param simulator: simulator instance\n",
    "    :param input_layer: input layer reference\n",
    "    :param output_layer: output layer reference\n",
    "    :param x_test: features of the testing subset\n",
    "    :param y_test: labels of the testing subset\n",
    "    :param network_name: name of the network\n",
    "    :return: accuracy, recall and precision metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate the remaining number of samples since the predict function does use minibatch\n",
    "    samples = (x_test.shape[0] // minibatch_size ) * minibatch_size\n",
    "    x_test, y_test = x_test[:samples], y_test[:samples]\n",
    "\n",
    "    predictions = simulator.predict(x_test)[output_layer] # get result from output layer when predicting on x_test\n",
    "    predictions = predictions[:,-1,:] # get the last timestep\n",
    "    predictions_argm = np.argmax(predictions, axis=-1) # get predicted label\n",
    "\n",
    "    y_test = np.squeeze(y_test, axis=1) # remove time dimension\n",
    "    y_test_argm = np.argmax(y_test, axis=-1) # get labels\n",
    "\n",
    "    precision = metrics.precision_score(y_true=y_test_argm, y_pred=predictions_argm, average='binary') # get precision score\n",
    "    recall = metrics.recall_score(y_true=y_test_argm, y_pred=predictions_argm, average='binary') # get recall\n",
    "    f1 = metrics.f1_score(y_true=y_test_argm, y_pred=predictions_argm, average='binary')\n",
    "    accuracy = metrics.accuracy_score(y_true=y_test_argm, y_pred=predictions_argm) # get accuracy\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true=y_test_argm, y_pred=predictions_argm)\n",
    "\n",
    "    # Log the statistics\n",
    "    print(f'{network_name}: accuracy = {accuracy * 100}%, precision = {precision}, '\n",
    "          f'recall = {recall}, f1 = {f1}')\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    return accuracy, precision, recall, f1, confusion_matrix\n",
    "\n",
    "def run_ann(model, train, test, params_save_path, iteration, shuffle_training=True, num_epochs=30):\n",
    "    \"\"\"\n",
    "    Run analog network with cross-validation\n",
    "    :param model: reference to the tensorflow model\n",
    "    :param train: pair of training data (x_train, y_train)\n",
    "    :param valid: pair of validation data (x_val, y_val)\n",
    "    :param test: pair of testing data (x_test, y_test)\n",
    "    :param params_save_path: output path to save weights of the network\n",
    "    :param iteration: number of the iteration in CV\n",
    "    :param shuffle_training: shuffle samples\n",
    "    :param num_epochs: number of epochs to train for\n",
    "    :return: accuracy, precision, recall, f1 and confusion matrix from the testing data\n",
    "    \"\"\"\n",
    "    x_train, y_train = train[0], train[1]\n",
    "    x_test, y_test = test[0], test[1]\n",
    "\n",
    "    converter = nengo_dl.Converter(model)\n",
    "\n",
    "    with nengo_dl.Simulator(converter.net) as simulator:\n",
    "        simulator.compile(optimizer=keras.optimizers.Adam(),\n",
    "                          loss=keras.losses.BinaryCrossentropy(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "        input_layer = converter.inputs[model.get_layer('input_layer')] # get the input layer reference\n",
    "        output_layer = converter.outputs[model.get_layer('output_layer')] # get the output layer reference\n",
    "\n",
    "        # fit the model with the training data\n",
    "        simulator.fit(\n",
    "            x={ input_layer: x_train }, y={ output_layer: y_train },\n",
    "            epochs=num_epochs,\n",
    "            shuffle=shuffle_training,\n",
    "            # early stop to avoid overfitting\n",
    "            callbacks=[EarlyStopping(patience=20, verbose=1, monitor='probe_loss', restore_best_weights=True)]\n",
    "        )\n",
    "\n",
    "        simulator.save_params(params_save_path) # save weights to the file\n",
    "\n",
    "        # Get statistics\n",
    "        accuracy, precision, recall, f1, confusion_matrix = get_metrics(simulator, output_layer, x_test, y_test, 16,\n",
    "                                                                        f'{iteration}. CNN')\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': confusion_matrix\n",
    "        }\n",
    "\n",
    "\n",
    "def run_snn(model, x_test, y_test, params_load_path, iteration, timesteps=50, scale_firing_rates=1000, synapse=0.01):\n",
    "    \"\"\"\n",
    "    Run model in spiking setting\n",
    "    :param model: model reference\n",
    "    :param x_test: testing features\n",
    "    :param y_test: testing labels\n",
    "    :param params_load_path: path to load parameters\n",
    "    :param iteration: number of current iteration\n",
    "    :param timesteps: number of timesteps\n",
    "    :param scale_firing_rates: firing rate scaling\n",
    "    :param synapse: synaptic smoothing\n",
    "    :return: accuracy, precision, recall, f1 and confusion matrix from the testing data\n",
    "    \"\"\"\n",
    "    converter = nengo_dl.Converter(\n",
    "        model,\n",
    "        swap_activations={ tf.nn.relu: nengo.SpikingRectifiedLinear() },\n",
    "        scale_firing_rates=scale_firing_rates,\n",
    "        synapse=synapse\n",
    "    ) # create a Nengo converter object and swap all relu activations with spiking relu\n",
    "\n",
    "    with converter.net:\n",
    "        nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    output_layer = converter.outputs[model.get_layer('output_layer')] # output layer for simulator\n",
    "\n",
    "    x_test_tiled = np.tile(x_test, (1, timesteps, 1)) # tile test data to timesteps\n",
    "\n",
    "    with nengo_dl.Simulator(converter.net) as simulator:\n",
    "        simulator.load_params(params_load_path)\n",
    "\n",
    "        # Get statistics\n",
    "        accuracy, precision, recall, f1, confusion_matrix = get_metrics(simulator, output_layer, x_test_tiled, y_test, 1,\n",
    "                                                                        f'{iteration}. CNN (SNN conversion)')\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': confusion_matrix\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reshape_dataset(features, labels):\n",
    "    \"\"\"\n",
    "    Function to reshape the dataset for it to be usable with Nengo\n",
    "    :param features: numpy array containing features\n",
    "    :param labels: numpy array containing labels\n",
    "    :return: transformed features and labels\n",
    "    \"\"\"\n",
    "    labels = labels.reshape((-1, 1))\n",
    "    labels = OneHotEncoder().fit_transform(labels).toarray()\n",
    "    labels = labels.reshape((labels.shape[0], 1, -1)) # flatten and add time dimension (necessary for nengo)\n",
    "    features = features.reshape((features.shape[0], 14, -1)) # reshape to channels x data\n",
    "    features = features.reshape((features.shape[0], 1, -1)) # flatten and add time dimension\n",
    "\n",
    "    return features, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_individual(file, particip_num, model, params_save_path, test_size=0.25, epochs=30, scale_firing_rates=1000,\n",
    "                   synapse=0.01, timesteps=50):\n",
    "    print(f'Running ANN and SNN for file: {file}')\n",
    "\n",
    "    dataset = np.load(file) # load numpy file containing the preprocessed data for specific participant\n",
    "    features, labels = dataset['features'], dataset['labels'] # get features and labels from the numpy file\n",
    "\n",
    "    # Transform numpy arrays to be usable with Nengo\n",
    "    features, labels = reshape_dataset(features, labels)\n",
    "\n",
    "    # Split the data into 75% training and 25% testing\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=seed,\n",
    "                                                        shuffle=True)\n",
    "    print('X (train) shape:', x_train.shape, 'Y (train) shape:', y_train.shape)\n",
    "    print('X (test) shape:', x_test.shape, 'Y (test) shape:', y_test.shape)\n",
    "\n",
    "    ann_stats = run_ann(model, (x_train, y_train), (x_test, y_test), params_save_path, particip_num, epochs=epochs)\n",
    "    snn_stats = run_snn(model, x_test,  y_test, params_save_path, particip_num,\n",
    "                      timesteps=timesteps, synapse=synapse, scale_firing_rates=scale_firing_rates)\n",
    "\n",
    "    return ann_stats, snn_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ann, snn = [], []\n",
    "participant_no = 1\n",
    "\n",
    "params_save_dir = 'cnn_individuals_nengo_params'\n",
    "os.makedirs(params_save_dir, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    file_name = 'P{:02d}'.format(participant_no)\n",
    "    model = cnn_model()\n",
    "    params_save_path = os.path.join(params_save_dir, file_name)\n",
    "\n",
    "    ann_stats, snn_stats = run_individual(file, participant_no, model, params_save_path)\n",
    "\n",
    "    ann.append(ann_stats) # append statistics to the list\n",
    "    snn.append(snn_stats)\n",
    "\n",
    "    participant_no += 1 # increase participant number\n",
    "\n",
    "    # Delete model and clear session to prevent memory leaks\n",
    "    K.clear_session()\n",
    "    del model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = {\n",
    "    'participant': [x for x in range(len(files))],\n",
    "    'ann_accuracy': [x['accuracy'] for x in ann],\n",
    "    'ann_precision': [x['precision'] for x in ann],\n",
    "    'ann_recall': [x['recall'] for x in ann],\n",
    "    'ann_f1': [x['f1'] for x in ann],\n",
    "    'snn_accuracy': [x['accuracy'] for x in snn],\n",
    "    'snn_precision': [x['precision'] for x in snn],\n",
    "    'snn_recall': [x['recall'] for x in snn],\n",
    "    'snn_f1': [x['f1'] for x in snn]\n",
    "}\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_output_folder = 'results' # output path for data from each iteration\n",
    "os.makedirs(data_output_folder, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(data) # create pandas dataframe and save it\n",
    "df.to_excel(os.path.join(data_output_folder, 'cnn_individuals.xlsx'))\n",
    "\n",
    "'Statistics for iterations successfully saved.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create statistics such as maximums and averages for each metric\n",
    "data_stats = {\n",
    "    'models': ['ann', 'snn'],\n",
    "    'average_accuracy': [],\n",
    "    'max_accuracy': [],\n",
    "    'accuracy_std': [],\n",
    "    'average_precision': [],\n",
    "    'max_precision': [],\n",
    "    'average_recall': [],\n",
    "    'max_recall': [],\n",
    "    'average_f1': [],\n",
    "    'max_f1': []\n",
    "}\n",
    "\n",
    "# slightly less code if we iterate over snn_{metric_name} in dictionary\n",
    "for model in ['ann', 'snn']:\n",
    "    data_stats['average_accuracy'].append(df[f'{model}_accuracy'].mean())\n",
    "    data_stats['accuracy_std'].append(df[f'{model}_accuracy'].std())\n",
    "    data_stats['average_precision'].append(df[f'{model}_precision'].mean())\n",
    "    data_stats['average_recall'].append(df[f'{model}_recall'].mean())\n",
    "    data_stats['average_f1'].append(df[f'{model}_f1'].mean())\n",
    "    data_stats['max_accuracy'].append(df[f'{model}_accuracy'].max())\n",
    "    data_stats['max_f1'].append(df[f'{model}_f1'].max())\n",
    "    data_stats['max_precision'].append(df[f'{model}_precision'].max())\n",
    "    data_stats['max_recall'].append(df[f'{model}_recall'].max())\n",
    "\n",
    "data_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dataframe for statistics and save it\n",
    "df_stats = pd.DataFrame(data_stats)\n",
    "df_stats.to_excel(os.path.join(data_output_folder, 'cnn_individuals_stats.xlsx'))\n",
    "\n",
    "'File with statistics successfully saved.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print confusion matrices for ANN\n",
    "conf_matrices_ann = [x['confusion_matrix'] for x in ann]\n",
    "print('Confusion matrices for the ANN:')\n",
    "for confusion_matrix in conf_matrices_ann:\n",
    "    print(confusion_matrix)\n",
    "\n",
    "# Print confusion matrices for SNN\n",
    "conf_matrices_snn = [x['confusion_matrix'] for x in snn]\n",
    "print('\\nConfusion matrices for the SNN')\n",
    "for confusion_matrix in conf_matrices_snn:\n",
    "    print(confusion_matrix)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}