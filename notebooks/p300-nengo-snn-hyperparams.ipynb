{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import statistics\n",
    "import nengo\n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from tensorflow.python.keras import Input, Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import Conv2D, BatchNormalization, Dropout, AveragePooling2D, Flatten, Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('..', 'datasets', 'VarekaGTNEpochs.mat') # path to dataset\n",
    "os.makedirs('nengo', exist_ok=True) # make folder for param files\n",
    "\n",
    "seed = 0 # set seed to produce same results\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(file = dataset_path):\n",
    "    \"\"\"\n",
    "    Helper function to get dataset from file.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    np_file = loadmat(file) # load dataset file with matrices\n",
    "    target_data, non_target_data = np_file['allTargetData'], np_file['allNonTargetData'] # get target and non-target data\n",
    "    features = np.concatenate((target_data, non_target_data)) # concatenate target and non-target into features\n",
    "\n",
    "    # target labels are represented as (1, 0) vector, non target labels are represented as (0, 1) vector\n",
    "    target_labels = np.tile(np.array([1, 0]), (target_data.shape[0], 1)) # set 'target' as (1, 0) vector\n",
    "    non_target_labels = np.tile(np.array([0, 1]), (non_target_data.shape[0], 1)) # set 'non target' as (0, 1) vector\n",
    "    labels = np.vstack((target_labels, non_target_labels)) # concatenate target and non target labels\n",
    "\n",
    "    # filter noise above 100 mV\n",
    "    threshold = 100.0\n",
    "    x_result, y_result = [], []\n",
    "    for i in range(features.shape[0]):\n",
    "        if not np.max(np.abs(features[i])) > threshold:\n",
    "            x_result.append(features[i])\n",
    "            y_result.append(labels[i])\n",
    "\n",
    "    features, labels = np.array(x_result), np.array(y_result)\n",
    "    features = features.reshape((features.shape[0], 1, -1))\n",
    "    labels = labels.reshape((labels.shape[0], 1, -1))\n",
    "\n",
    "    print('features:', features, features.shape)\n",
    "    print('labels:', labels, labels.shape)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Function to create tensorflow model\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(3, 1200, 1), name='input_layer')\n",
    "    conv2d = Conv2D(filters=6, kernel_size=(3, 3), activation=tf.nn.relu)(inp)\n",
    "    dropout1 = Dropout(0.5, seed=0)(conv2d)\n",
    "    avg_pooling = AveragePooling2D(pool_size=(1, 8), padding='same')(dropout1)\n",
    "    flatten = Flatten()(avg_pooling)\n",
    "    dense1 = Dense(100, activation=tf.nn.relu)(flatten)\n",
    "    batch_norm = BatchNormalization()(dense1)\n",
    "    dropout2 = Dropout(0.5, seed=0)(batch_norm)\n",
    "    output = Dense(2, activation=tf.nn.softmax, name='output_layer')(dropout2)\n",
    "\n",
    "    return Model(inputs=inp, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_ann(model, train, valid, test, params_save_path, iteration, shuffle_training=True):\n",
    "    \"\"\"\n",
    "    Run ann via Nengo simulator. This fits given model with training data (train) and validates it using validation\n",
    "    data (valid). Then accuracy is calculated using test data (test) and weights are saved to params_save_path\n",
    "    :param shuffle_training: whether to shuffle data\n",
    "    :param model: tensorflow model created from create_model() function\n",
    "    :param train: pair of features and labels from training data\n",
    "    :param valid: pair of features and labels from validation data\n",
    "    :param test: pair of features and labels from test data\n",
    "    :param params_save_path: output path to save weights of the network for SNN testing\n",
    "    :return accuracy on test data\n",
    "    \"\"\"\n",
    "    x_train, y_train = train[0], train[1]\n",
    "    x_valid, y_valid = valid[0], valid[1]\n",
    "    x_test, y_test = test[0], test[1]\n",
    "\n",
    "    converter = nengo_dl.Converter(model)\n",
    "    with nengo_dl.Simulator(converter.net, minibatch_size=16) as simulator:\n",
    "        # some data will get truncated due to batch size\n",
    "        simulator.compile(\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            loss=keras.losses.BinaryCrossentropy(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        input_layer = converter.inputs[model.get_layer('input_layer')] # get nengo input\n",
    "        output_layer = converter.outputs[model.get_layer('output_layer')] # get nengo output\n",
    "\n",
    "        simulator.fit(\n",
    "            x={ input_layer: x_train }, y={ output_layer: y_train },\n",
    "            validation_data=({ input_layer: x_valid }, { output_layer: y_valid }),\n",
    "            epochs=30,\n",
    "            shuffle=shuffle_training,\n",
    "            callbacks=[EarlyStopping(patience=5, verbose=1, restore_best_weights=True)] # early stop to avoid overfitting\n",
    "        ) # train model\n",
    "\n",
    "        simulator.save_params(params_save_path) # save params for SNN\n",
    "        ann_eval = simulator.evaluate(x={ input_layer: x_test }, y={ output_layer: y_test }) # evaluate accuracy\n",
    "        print('{}. ann accuracy: {:5f}%'.format(iteration, ann_eval['probe_accuracy'] * 100)) # log accuracy\n",
    "        return ann_eval['probe_accuracy'] # return accuracy\n",
    "\n",
    "def run_snn(model, test, params_load_path, timesteps, scale_firing_rates, synapse, iteration):\n",
    "    \"\"\"\n",
    "    Runs SNN on test data. Loads pre-trained weights from params_load path and uses timesteps, scale_firing_rates and synapse\n",
    "    parameters for simulator.\n",
    "    :param model:\n",
    "    :param test:\n",
    "    :param params_load_path:\n",
    "    :param timesteps:\n",
    "    :param scale_firing_rates:\n",
    "    :param synapse:\n",
    "    :param iteration:\n",
    "    :return: accuracy on predicted data\n",
    "    \"\"\"\n",
    "    converter = nengo_dl.Converter(\n",
    "        model=model,\n",
    "        swap_activations={ tf.nn.relu: nengo.SpikingRectifiedLinear() },\n",
    "        scale_firing_rates=scale_firing_rates,\n",
    "        synapse=synapse\n",
    "    )\n",
    "\n",
    "    x_test, y_test = test[0], test[1]\n",
    "\n",
    "    with converter.net:\n",
    "        nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    input_layer = converter.inputs[model.get_layer('input_layer')] # input layer for simulator\n",
    "    output_layer = converter.outputs[model.get_layer('output_layer')] # output layer for simulator\n",
    "\n",
    "    x_test_time_tiled = np.tile(x_test, (1, timesteps, 1)) # tile x_test to match desired timesteps for simulator\n",
    "\n",
    "    with nengo_dl.Simulator(converter.net, minibatch_size=41, progress_bar=False) as simulator:\n",
    "        simulator.load_params(params_load_path)\n",
    "\n",
    "        predictions = simulator.predict({ input_layer: x_test_time_tiled })[output_layer] # get results from prediction\n",
    "        predictions = predictions[:,-1,:] # get last timestep\n",
    "\n",
    "        predictions = np.argmax(predictions, axis=-1) # get argmax\n",
    "        y_test = np.squeeze(y_test, axis=1) # remove time dimension from labels since its not relevant\n",
    "        y_test = np.argmax(y_test, axis=-1) # get argmax of y test as well for comparison\n",
    "\n",
    "        snn_avg = (predictions == y_test).mean()\n",
    "\n",
    "        if synapse is None:\n",
    "            print('{}. snn [timesteps={}, scale_firing_rates={}, synapse=None], accuracy: {:4f}%'\n",
    "                .format(iteration, timesteps, scale_firing_rates, snn_avg*100))\n",
    "        else:\n",
    "            print('{}. snn [timesteps={}, scale_firing_rates={}, synapse={:5f}], accuracy: {:4f}%'\n",
    "                .format(iteration, timesteps, scale_firing_rates, synapse, snn_avg*100))\n",
    "\n",
    "        return snn_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9ms/step - loss: 0.6376 - probe_loss: 0.6376 - probe_accuracy: 0.6376 - val_loss: 0.6538 - val_probe_loss: 0.6538 - val_probe_accuracy: 0.6396\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - 2s 9ms/step - loss: 0.6380 - probe_loss: 0.6380 - probe_accuracy: 0.6407 - val_loss: 0.6532 - val_probe_loss: 0.6532 - val_probe_accuracy: 0.6310\n",
      "Epoch 10/30\n",
      "277/282 [============================>.] - ETA: 0s - loss: 0.6379 - probe_loss: 0.6379 - probe_accuracy: 0.6374Restoring model weights from the end of the best epoch.\n",
      "282/282 [==============================] - 2s 9ms/step - loss: 0.6376 - probe_loss: 0.6376 - probe_accuracy: 0.6383 - val_loss: 0.6580 - val_probe_loss: 0.6580 - val_probe_accuracy: 0.6283\n",
      "Epoch 00010: early stopping\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.6372 - probe_loss: 0.6372 - probe_accuracy: 0.6510\n",
      "25. ann accuracy: 65.100002%\n",
      "25. snn [timesteps=50, scale_firing_rates=1000, synapse=0.010000], accuracy: 65.156794%\n",
      "25. snn [timesteps=50, scale_firing_rates=1, synapse=0.010000], accuracy: 54.454953%\n",
      "25. snn [timesteps=50, scale_firing_rates=1000, synapse=None], accuracy: 65.256346%\n",
      "25. snn [timesteps=50, scale_firing_rates=1, synapse=None], accuracy: 53.111000%\n",
      "Iteration: 26\n",
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/30\n",
      "282/282 [==============================] - 3s 12ms/step - loss: 0.9279 - probe_loss: 0.9279 - probe_accuracy: 0.5718 - val_loss: 0.7536 - val_probe_loss: 0.7536 - val_probe_accuracy: 0.6017\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.7169 - probe_loss: 0.7169 - probe_accuracy: 0.5942 - val_loss: 0.6954 - val_probe_loss: 0.6954 - val_probe_accuracy: 0.5938\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6808 - probe_loss: 0.6808 - probe_accuracy: 0.6051 - val_loss: 0.6707 - val_probe_loss: 0.6707 - val_probe_accuracy: 0.5971\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6544 - probe_loss: 0.6544 - probe_accuracy: 0.6257 - val_loss: 0.6644 - val_probe_loss: 0.6644 - val_probe_accuracy: 0.5957\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6457 - probe_loss: 0.6457 - probe_accuracy: 0.6323 - val_loss: 0.6618 - val_probe_loss: 0.6618 - val_probe_accuracy: 0.6004\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6473 - probe_loss: 0.6473 - probe_accuracy: 0.6425 - val_loss: 0.6535 - val_probe_loss: 0.6535 - val_probe_accuracy: 0.6057\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6433 - probe_loss: 0.6433 - probe_accuracy: 0.6392 - val_loss: 0.6542 - val_probe_loss: 0.6542 - val_probe_accuracy: 0.6130\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.6418 - probe_loss: 0.6418 - probe_accuracy: 0.6345 - val_loss: 0.6536 - val_probe_loss: 0.6536 - val_probe_accuracy: 0.6117\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6350 - probe_loss: 0.6350 - probe_accuracy: 0.6454 - val_loss: 0.6573 - val_probe_loss: 0.6573 - val_probe_accuracy: 0.6051\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6395 - probe_loss: 0.6395 - probe_accuracy: 0.6421 - val_loss: 0.6606 - val_probe_loss: 0.6606 - val_probe_accuracy: 0.6164\n",
      "Epoch 11/30\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6358 - probe_loss: 0.6358 - probe_accuracy: 0.6421Restoring model weights from the end of the best epoch.\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6357 - probe_loss: 0.6357 - probe_accuracy: 0.6421 - val_loss: 0.6537 - val_probe_loss: 0.6537 - val_probe_accuracy: 0.6104\n",
      "Epoch 00011: early stopping\n",
      "  1/125 [..............................] - ETA: 0s - loss: 0.6680 - probe_loss: 0.6680 - probe_accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0030s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0030s). Check your callbacks.\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.6451 - probe_loss: 0.6451 - probe_accuracy: 0.6375\n",
      "26. ann accuracy: 63.749999%\n",
      "26. snn [timesteps=50, scale_firing_rates=1000, synapse=0.010000], accuracy: 63.812842%\n",
      "26. snn [timesteps=50, scale_firing_rates=1, synapse=0.010000], accuracy: 52.613240%\n",
      "26. snn [timesteps=50, scale_firing_rates=1000, synapse=None], accuracy: 63.912394%\n",
      "26. snn [timesteps=50, scale_firing_rates=1, synapse=None], accuracy: 51.567944%\n",
      "Iteration: 27\n",
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/30\n",
      "282/282 [==============================] - 4s 12ms/step - loss: 0.9438 - probe_loss: 0.9438 - probe_accuracy: 0.5361 - val_loss: 0.7881 - val_probe_loss: 0.7881 - val_probe_accuracy: 0.5685\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.7471 - probe_loss: 0.7471 - probe_accuracy: 0.5583 - val_loss: 0.6676 - val_probe_loss: 0.6676 - val_probe_accuracy: 0.6144\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6771 - probe_loss: 0.6771 - probe_accuracy: 0.5991 - val_loss: 0.6595 - val_probe_loss: 0.6595 - val_probe_accuracy: 0.6124\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6668 - probe_loss: 0.6668 - probe_accuracy: 0.6079 - val_loss: 0.6570 - val_probe_loss: 0.6570 - val_probe_accuracy: 0.6177\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6645 - probe_loss: 0.6645 - probe_accuracy: 0.6024 - val_loss: 0.6639 - val_probe_loss: 0.6639 - val_probe_accuracy: 0.6024\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6525 - probe_loss: 0.6525 - probe_accuracy: 0.6210 - val_loss: 0.6904 - val_probe_loss: 0.6904 - val_probe_accuracy: 0.5964\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6534 - probe_loss: 0.6534 - probe_accuracy: 0.6184 - val_loss: 0.6555 - val_probe_loss: 0.6555 - val_probe_accuracy: 0.6283\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6490 - probe_loss: 0.6490 - probe_accuracy: 0.6234 - val_loss: 0.6523 - val_probe_loss: 0.6523 - val_probe_accuracy: 0.6350\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6518 - probe_loss: 0.6518 - probe_accuracy: 0.6266 - val_loss: 0.6551 - val_probe_loss: 0.6551 - val_probe_accuracy: 0.6263\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6467 - probe_loss: 0.6467 - probe_accuracy: 0.6297 - val_loss: 0.6493 - val_probe_loss: 0.6493 - val_probe_accuracy: 0.6290\n",
      "Epoch 11/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6450 - probe_loss: 0.6450 - probe_accuracy: 0.6299 - val_loss: 0.6529 - val_probe_loss: 0.6529 - val_probe_accuracy: 0.6230\n",
      "Epoch 12/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6429 - probe_loss: 0.6429 - probe_accuracy: 0.6328 - val_loss: 0.6619 - val_probe_loss: 0.6619 - val_probe_accuracy: 0.6203\n",
      "Epoch 13/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6420 - probe_loss: 0.6420 - probe_accuracy: 0.6345 - val_loss: 0.6503 - val_probe_loss: 0.6503 - val_probe_accuracy: 0.6177\n",
      "Epoch 14/30\n",
      "282/282 [==============================] - 2s 9ms/step - loss: 0.6403 - probe_loss: 0.6403 - probe_accuracy: 0.6376 - val_loss: 0.6494 - val_probe_loss: 0.6494 - val_probe_accuracy: 0.6283\n",
      "Epoch 15/30\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.6404 - probe_loss: 0.6404 - probe_accuracy: 0.6394Restoring model weights from the end of the best epoch.\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6387 - probe_loss: 0.6387 - probe_accuracy: 0.6412 - val_loss: 0.6522 - val_probe_loss: 0.6522 - val_probe_accuracy: 0.6316\n",
      "Epoch 00015: early stopping\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.6482 - probe_loss: 0.6482 - probe_accuracy: 0.6420\n",
      "27. ann accuracy: 64.200002%\n",
      "27. snn [timesteps=50, scale_firing_rates=1000, synapse=0.010000], accuracy: 64.459930%\n",
      "27. snn [timesteps=50, scale_firing_rates=1, synapse=0.010000], accuracy: 53.011448%\n",
      "27. snn [timesteps=50, scale_firing_rates=1000, synapse=None], accuracy: 64.509706%\n",
      "27. snn [timesteps=50, scale_firing_rates=1, synapse=None], accuracy: 53.758089%\n",
      "Iteration: 28\n",
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/30\n",
      "282/282 [==============================] - 3s 12ms/step - loss: 0.8968 - probe_loss: 0.8968 - probe_accuracy: 0.5472 - val_loss: 0.7009 - val_probe_loss: 0.7009 - val_probe_accuracy: 0.5944\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.7208 - probe_loss: 0.7208 - probe_accuracy: 0.5827 - val_loss: 0.6633 - val_probe_loss: 0.6633 - val_probe_accuracy: 0.6157\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6847 - probe_loss: 0.6847 - probe_accuracy: 0.5955 - val_loss: 0.6705 - val_probe_loss: 0.6705 - val_probe_accuracy: 0.5918\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6640 - probe_loss: 0.6640 - probe_accuracy: 0.6066 - val_loss: 0.6624 - val_probe_loss: 0.6624 - val_probe_accuracy: 0.6077\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6595 - probe_loss: 0.6595 - probe_accuracy: 0.6139 - val_loss: 0.6598 - val_probe_loss: 0.6598 - val_probe_accuracy: 0.6124\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6560 - probe_loss: 0.6560 - probe_accuracy: 0.6079 - val_loss: 0.6710 - val_probe_loss: 0.6710 - val_probe_accuracy: 0.6024\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6538 - probe_loss: 0.6538 - probe_accuracy: 0.6168 - val_loss: 0.6548 - val_probe_loss: 0.6548 - val_probe_accuracy: 0.6323\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6504 - probe_loss: 0.6504 - probe_accuracy: 0.6314 - val_loss: 0.6557 - val_probe_loss: 0.6557 - val_probe_accuracy: 0.6330\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6448 - probe_loss: 0.6448 - probe_accuracy: 0.6330 - val_loss: 0.6616 - val_probe_loss: 0.6616 - val_probe_accuracy: 0.6137\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6431 - probe_loss: 0.6431 - probe_accuracy: 0.6341 - val_loss: 0.6507 - val_probe_loss: 0.6507 - val_probe_accuracy: 0.6263\n",
      "Epoch 11/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6423 - probe_loss: 0.6423 - probe_accuracy: 0.6321 - val_loss: 0.6520 - val_probe_loss: 0.6520 - val_probe_accuracy: 0.6310\n",
      "Epoch 12/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6385 - probe_loss: 0.6385 - probe_accuracy: 0.6454 - val_loss: 0.6638 - val_probe_loss: 0.6638 - val_probe_accuracy: 0.6070\n",
      "Epoch 13/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6411 - probe_loss: 0.6411 - probe_accuracy: 0.6412 - val_loss: 0.6576 - val_probe_loss: 0.6576 - val_probe_accuracy: 0.6090\n",
      "Epoch 14/30\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.6379 - probe_loss: 0.6379 - probe_accuracy: 0.6509 - val_loss: 0.6589 - val_probe_loss: 0.6589 - val_probe_accuracy: 0.6064\n",
      "Epoch 15/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6379 - probe_loss: 0.6379 - probe_accuracy: 0.6374Restoring model weights from the end of the best epoch.\n",
      "282/282 [==============================] - 2s 9ms/step - loss: 0.6379 - probe_loss: 0.6379 - probe_accuracy: 0.6374 - val_loss: 0.6580 - val_probe_loss: 0.6580 - val_probe_accuracy: 0.6164\n",
      "Epoch 00015: early stopping\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.6460 - probe_loss: 0.6460 - probe_accuracy: 0.6500\n",
      "28. ann accuracy: 64.999998%\n",
      "28. snn [timesteps=50, scale_firing_rates=1000, synapse=0.010000], accuracy: 64.907914%\n",
      "28. snn [timesteps=50, scale_firing_rates=1, synapse=0.010000], accuracy: 53.758089%\n",
      "28. snn [timesteps=50, scale_firing_rates=1000, synapse=None], accuracy: 64.907914%\n",
      "28. snn [timesteps=50, scale_firing_rates=1, synapse=None], accuracy: 52.812344%\n",
      "Iteration: 29\n",
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/30\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.9843 - probe_loss: 0.9843 - probe_accuracy: 0.5592 - val_loss: 0.7357 - val_probe_loss: 0.7357 - val_probe_accuracy: 0.6170\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.7666 - probe_loss: 0.7666 - probe_accuracy: 0.5714 - val_loss: 0.6722 - val_probe_loss: 0.6722 - val_probe_accuracy: 0.6104\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6959 - probe_loss: 0.6959 - probe_accuracy: 0.5946 - val_loss: 0.6717 - val_probe_loss: 0.6717 - val_probe_accuracy: 0.6144\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.6664 - probe_loss: 0.6664 - probe_accuracy: 0.6152 - val_loss: 0.6555 - val_probe_loss: 0.6555 - val_probe_accuracy: 0.6117\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6589 - probe_loss: 0.6589 - probe_accuracy: 0.6192 - val_loss: 0.6586 - val_probe_loss: 0.6586 - val_probe_accuracy: 0.6164\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6522 - probe_loss: 0.6522 - probe_accuracy: 0.6179 - val_loss: 0.6513 - val_probe_loss: 0.6513 - val_probe_accuracy: 0.6203\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6465 - probe_loss: 0.6465 - probe_accuracy: 0.6272 - val_loss: 0.6517 - val_probe_loss: 0.6517 - val_probe_accuracy: 0.6170\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6479 - probe_loss: 0.6479 - probe_accuracy: 0.6254 - val_loss: 0.6464 - val_probe_loss: 0.6464 - val_probe_accuracy: 0.6223\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6464 - probe_loss: 0.6464 - probe_accuracy: 0.6332 - val_loss: 0.6590 - val_probe_loss: 0.6590 - val_probe_accuracy: 0.6137\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6360 - probe_loss: 0.6360 - probe_accuracy: 0.6527 - val_loss: 0.6648 - val_probe_loss: 0.6648 - val_probe_accuracy: 0.6090\n",
      "Epoch 11/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6390 - probe_loss: 0.6390 - probe_accuracy: 0.6387 - val_loss: 0.6598 - val_probe_loss: 0.6598 - val_probe_accuracy: 0.6157\n",
      "Epoch 12/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6432 - probe_loss: 0.6432 - probe_accuracy: 0.6314 - val_loss: 0.6519 - val_probe_loss: 0.6519 - val_probe_accuracy: 0.6184\n",
      "Epoch 13/30\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.6351 - probe_loss: 0.6351 - probe_accuracy: 0.6457Restoring model weights from the end of the best epoch.\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6345 - probe_loss: 0.6345 - probe_accuracy: 0.6461 - val_loss: 0.6568 - val_probe_loss: 0.6568 - val_probe_accuracy: 0.6184\n",
      "Epoch 00013: early stopping\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.6434 - probe_loss: 0.6434 - probe_accuracy: 0.6355\n",
      "29. ann accuracy: 63.550001%\n",
      "29. snn [timesteps=50, scale_firing_rates=1000, synapse=0.010000], accuracy: 63.663514%\n",
      "29. snn [timesteps=50, scale_firing_rates=1, synapse=0.010000], accuracy: 52.463912%\n",
      "29. snn [timesteps=50, scale_firing_rates=1000, synapse=None], accuracy: 63.663514%\n",
      "29. snn [timesteps=50, scale_firing_rates=1, synapse=None], accuracy: 51.767048%\n",
      "Iteration: 30\n",
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/30\n",
      "282/282 [==============================] - 3s 12ms/step - loss: 0.9062 - probe_loss: 0.9062 - probe_accuracy: 0.5494 - val_loss: 0.7059 - val_probe_loss: 0.7059 - val_probe_accuracy: 0.5785\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.7226 - probe_loss: 0.7226 - probe_accuracy: 0.5840 - val_loss: 0.6797 - val_probe_loss: 0.6797 - val_probe_accuracy: 0.5991\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6673 - probe_loss: 0.6673 - probe_accuracy: 0.6155 - val_loss: 0.6604 - val_probe_loss: 0.6604 - val_probe_accuracy: 0.6277\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6589 - probe_loss: 0.6589 - probe_accuracy: 0.6119 - val_loss: 0.6604 - val_probe_loss: 0.6604 - val_probe_accuracy: 0.6270\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6466 - probe_loss: 0.6466 - probe_accuracy: 0.6268 - val_loss: 0.6560 - val_probe_loss: 0.6560 - val_probe_accuracy: 0.6310\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6447 - probe_loss: 0.6447 - probe_accuracy: 0.6367 - val_loss: 0.6587 - val_probe_loss: 0.6587 - val_probe_accuracy: 0.6164\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6420 - probe_loss: 0.6420 - probe_accuracy: 0.6407 - val_loss: 0.6569 - val_probe_loss: 0.6569 - val_probe_accuracy: 0.6197\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6438 - probe_loss: 0.6438 - probe_accuracy: 0.6305 - val_loss: 0.6617 - val_probe_loss: 0.6617 - val_probe_accuracy: 0.6223\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6442 - probe_loss: 0.6442 - probe_accuracy: 0.6312 - val_loss: 0.6548 - val_probe_loss: 0.6548 - val_probe_accuracy: 0.6297\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6424 - probe_loss: 0.6424 - probe_accuracy: 0.6334 - val_loss: 0.6611 - val_probe_loss: 0.6611 - val_probe_accuracy: 0.6383\n",
      "Epoch 11/30\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.6352 - probe_loss: 0.6352 - probe_accuracy: 0.6421 - val_loss: 0.6600 - val_probe_loss: 0.6600 - val_probe_accuracy: 0.6257\n",
      "Epoch 12/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6370 - probe_loss: 0.6370 - probe_accuracy: 0.6396 - val_loss: 0.6558 - val_probe_loss: 0.6558 - val_probe_accuracy: 0.6297\n",
      "Epoch 13/30\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6345 - probe_loss: 0.6345 - probe_accuracy: 0.6485 - val_loss: 0.6647 - val_probe_loss: 0.6647 - val_probe_accuracy: 0.6283\n",
      "Epoch 14/30\n",
      "276/282 [============================>.] - ETA: 0s - loss: 0.6379 - probe_loss: 0.6379 - probe_accuracy: 0.6409Restoring model weights from the end of the best epoch.\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.6367 - probe_loss: 0.6367 - probe_accuracy: 0.6416 - val_loss: 0.6671 - val_probe_loss: 0.6671 - val_probe_accuracy: 0.6184\n",
      "Epoch 00014: early stopping\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.6439 - probe_loss: 0.6439 - probe_accuracy: 0.6435\n",
      "30. ann accuracy: 64.349997%\n",
      "30. snn [timesteps=50, scale_firing_rates=1000, synapse=0.010000], accuracy: 64.459930%\n",
      "30. snn [timesteps=50, scale_firing_rates=1, synapse=0.010000], accuracy: 55.350921%\n",
      "30. snn [timesteps=50, scale_firing_rates=1000, synapse=None], accuracy: 64.410154%\n",
      "30. snn [timesteps=50, scale_firing_rates=1, synapse=None], accuracy: 53.459433%\n"
     ]
    }
   ],
   "source": [
    "features, labels = get_dataset()\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=.25, random_state=seed, shuffle=True)\n",
    "\n",
    "print('train_x: {}, shape: {}, train_y: {}, shape: {}'.format(x_train, x_train.shape, y_train, y_train.shape))\n",
    "print('test_x: {}, shape: {}, test_y: {}, shape: {}'.format(x_test, x_test.shape, y_test, y_test.shape))\n",
    "\n",
    "ann, snn = [], []\n",
    "\n",
    "# 'hyper-parameters' with format: timesteps, scale_firing_rates, synapse\n",
    "snn_config = [\n",
    "    [50, 1000, 0.01], # best performing parameters for simulator\n",
    "    [50, 1, 0.01], # spike scaling fire rates are set to 1 == off\n",
    "    [50, 1000, None], # synaptic smoothing turned off\n",
    "    [50, 1, None] # everything turned off, only RELU is swapped for spiking RELU\n",
    "]\n",
    "\n",
    "n_iterations = 30 # iterations of cross-valiation\n",
    "iteration = 1 # number of current iteration\n",
    "monte_carlo = ShuffleSplit(n_splits=n_iterations, test_size=.25, random_state=seed)\n",
    "for train, valid in monte_carlo.split(x_train):\n",
    "    print('Iteration: {}'.format(iteration)) # Print number of iteration\n",
    "\n",
    "    x_train_current, y_train_current = x_train[train], y_train[train] # get training data for current iteration\n",
    "    x_valid_current, y_valid_current = x_train[valid], y_train[valid] # get validation data for current iteration\n",
    "\n",
    "    params_path = os.path.join('nengo', 'params_iter_{}'.format(iteration)) # path to weights for current iteration\n",
    "\n",
    "    model = create_model() # create model\n",
    "\n",
    "    # run ann\n",
    "    ann_result = run_ann(model=model,\n",
    "                         train=(x_train_current, y_train_current),\n",
    "                         valid=(x_valid_current, y_valid_current),\n",
    "                         test=(x_test, y_test),\n",
    "                         params_save_path=params_path,\n",
    "                         iteration=iteration\n",
    "                         )\n",
    "    K.clear_session() # clear session\n",
    "    ann.append(ann_result)\n",
    "\n",
    "    # run spiking network for each combination of timesteps, spike scaling and synapse and append it to the snn_results\n",
    "    snn_results = []\n",
    "    for variant in snn_config:\n",
    "        snn_acc = run_snn(model=model,\n",
    "                          test=(x_test, y_test),\n",
    "                          params_load_path=params_path,\n",
    "                          timesteps=variant[0],\n",
    "                          scale_firing_rates=variant[1],\n",
    "                          synapse=variant[2],\n",
    "                          iteration=iteration\n",
    "                          )\n",
    "        K.clear_session() # clear session not to cause a memory leak\n",
    "        snn_results.append(snn_acc)\n",
    "\n",
    "    del model # delete model to avoid potential memory leak (this might be unnecessary)\n",
    "    iteration += 1\n",
    "    # append list of accuracies for each parameter variant to the snn list\n",
    "    snn.append(snn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_stats(ann, snn, snn_config, snn_config_output, xlsx_output, average_accs_output):\n",
    "    \"\"\"\n",
    "    Save statistics to excel file. These were further used in p300-nengo-visualization notebook\n",
    "    :param ann: accuracies of ann network\n",
    "    :param snn: accuracies of snn network\n",
    "    :param snn_config: list of configurations for snn\n",
    "    :param snn_config_output: name of json file where configs for snn will be saved\n",
    "    :param xlsx_output: name of output file\n",
    "    :param average_accs_output: name of file containing average accuracy and std\n",
    "    \"\"\"\n",
    "\n",
    "    snn_config_array = []\n",
    "    for variant in snn_config:\n",
    "        snn_config_array.append({\n",
    "            'timesteps': variant[0],\n",
    "            'scale_firing_rates': variant[1],\n",
    "            'synapse': variant[2] if variant[2] is not None else 'None'\n",
    "        })\n",
    "\n",
    "    with open(snn_config_output, 'w') as file:\n",
    "        json.dump(snn_config_array, file)\n",
    "\n",
    "    data = {\n",
    "        'iterations': [x for x in range(1, len(ann) + 1)],\n",
    "        'ann_accuracy': ann,\n",
    "    } # data dictionary for pandas dataframe\n",
    "\n",
    "    variants = [] # create key for data dictionary for each configuration of snn\n",
    "    for variant in snn_config:\n",
    "        # synapse can be None which is cannot be passed as format parameter for some reason\n",
    "        if variant[2] is None:\n",
    "            variants.append('snn [timesteps={}, scaling={}, synapse=None]'\n",
    "                            .format(variant[0], int(variant[1])))\n",
    "        else:\n",
    "            variants.append('snn [timesteps={}, scaling={}, synapse={:3f}]'\n",
    "                        .format(variant[0], int(variant[1]), variant[2]))\n",
    "\n",
    "    # split accuracies for each configuration to specific list\n",
    "    # meaning each accuracy with configuration \"A\" will be in list on index a ...\n",
    "    snn_accs_by_variant = [[] for _ in range(len(variants))]\n",
    "    for snn_acc_list in snn:\n",
    "        for i in range(len(snn_acc_list)):\n",
    "            snn_accs_by_variant[i].append(snn_acc_list[i])\n",
    "\n",
    "    # add accuracies for each snn configuration to the dataset\n",
    "    for i in range(len(variants)):\n",
    "        data[variants[i]] = snn_accs_by_variant[i]\n",
    "\n",
    "    # create dataframe, print it and save as excel spreadsheet\n",
    "    df = pd.DataFrame(data=data, index=list(range(1, len(ann) + 1)))\n",
    "    print(df)\n",
    "    df.to_excel(xlsx_output)\n",
    "\n",
    "    # create dataframe for average accuracies and their sample standard deviations\n",
    "\n",
    "    data_stats = {\n",
    "        'model': ['ANN'] + variants,\n",
    "        'average_accuracy': [],\n",
    "        'sample_standard_deviation': []\n",
    "    }\n",
    "\n",
    "    data_stats['average_accuracy'].append(statistics.mean(ann))\n",
    "    data_stats['sample_standard_deviation'].append(statistics.stdev(ann))\n",
    "\n",
    "    for snn_acc in snn_accs_by_variant:\n",
    "        data_stats['average_accuracy'].append(statistics.mean(snn_acc))\n",
    "        data_stats['sample_standard_deviation'].append(statistics.stdev(snn_acc))\n",
    "    \n",
    "    df_stats = pd.DataFrame(data=data_stats) # create dataframe for stats\n",
    "    df_stats.to_excel(average_accs_output, index=False) # save to excel\n",
    "\n",
    "    #\n",
    "    # # create graph. Graph will be 130% of default size to ensure readability since it contains a lot of data\n",
    "    # df.plot(x='iterations', y=['ann_accuracy'] + variants, kind='bar', title='Accuracy of ANN and respective variant of SNN per iteration',\n",
    "    #         figsize=[6.4 * 3, 4.8 * 3], width=0.9)\n",
    "    # plt.yticks(np.arange(0, 1.0, 0.05))\n",
    "    # plt.xticks(rotation=0)\n",
    "    # plt.ylabel('Accuracy')\n",
    "    # plt.xlabel('Iterations')\n",
    "    # # plt.legend(loc='upper left', bbox_to_anchor=(1.01, 1), borderaxespad=0.)\n",
    "    # plt.legend(loc='lower left', bbox_to_anchor= (0.0, 1.05), ncol=3,\n",
    "    #         borderaxespad=0, frameon=False)\n",
    "    # plt.savefig(graph_output, format=graph_format, bbox_inches='tight')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "save_stats() got an unexpected keyword argument 'save_stats'",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-99e360c257d8>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m save_stats(ann=ann,\n\u001B[0m\u001B[0;32m      2\u001B[0m           \u001B[0msnn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msnn\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m           \u001B[0msnn_config\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msnn_config\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m           \u001B[0msave_stats\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'output'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'snn_config.json'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m           \u001B[0mxlsx_output\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'output'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'values.xlsx'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: save_stats() got an unexpected keyword argument 'save_stats'"
     ]
    }
   ],
   "source": [
    "save_stats(ann=ann,\n",
    "          snn=snn,\n",
    "          snn_config=snn_config,\n",
    "          snn_config_output=os.path.join('output', 'snn_config.json'),\n",
    "          xlsx_output=os.path.join('output', 'values.xlsx'),\n",
    "          average_accs_output=os.path.join('output','accs.xlsx')\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (use-of-snn)",
   "language": "python",
   "name": "pycharm-9c9070f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}