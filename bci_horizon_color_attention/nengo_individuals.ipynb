{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook uses data from each participant individually. A spiking neural network is created for every participant\n",
    "and then tested on 25% of the participant's data which were previously not used for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import nengo_dl\n",
    "from tensorflow.python.keras import Input, Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import Conv2D, Dropout, AveragePooling2D, Flatten, Dense, BatchNormalization, Conv3D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('dataset_result')\n",
    "files = [os.path.join(dataset_path, 'P{:02d}.npz'.format(i+1))\n",
    "         for i in range(18)] # P01 - P18 files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we check data for the first file,\n",
    "these operations are later applied for every file of the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 14, 36, 10)\n",
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "# Check data in the first file then apply it for every file\n",
    "dataset = np.load(files[0])\n",
    "features, labels = dataset['features'], dataset['labels']\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no' 'yes' 'no']\n"
     ]
    }
   ],
   "source": [
    "print(labels[50:53]) # no, yes, no"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[[1. 0.]]\n",
      "\n",
      " [[0. 1.]]\n",
      "\n",
      " [[0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "cat = OneHotEncoder()\n",
    "labels = labels.reshape(-1, 1)\n",
    "print(labels[50:53])\n",
    "labels = cat.fit_transform(labels).toarray()\n",
    "print(labels[50:53])\n",
    "labels = labels.reshape((labels.shape[0], 1, -1)) # add time dimension for nengo\n",
    "print(labels[50:53])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now define tensorflow models which will be tested\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cnn_model_1():\n",
    "    \"\"\"\n",
    "    Creates a CNN neural network\n",
    "    :return: tensorflow model of the ANN\n",
    "    \"\"\"\n",
    "\n",
    "    inp = Input(shape=(14, 360, 1), name='input_layer')\n",
    "    conv2d = Conv2D(filters=9, kernel_size=(3, 3), activation='relu')(inp)\n",
    "    dropout1 = Dropout(0.5, seed=seed)(conv2d)\n",
    "    avg_pooling = AveragePooling2D(pool_size=(2, 2))(dropout1)\n",
    "    flatten = Flatten()(avg_pooling)\n",
    "    dense1 = Dense(1000, activation='relu')(flatten)\n",
    "    batch_norm = BatchNormalization()(dense1)\n",
    "    dense2 = Dense(500, activation='relu')(batch_norm)\n",
    "    dropout2 = Dropout(0.5, seed=seed)(dense2)\n",
    "    output = Dense(2, activation='softmax', name='output_layer')(dropout2)\n",
    "\n",
    "    return Model(inputs=inp, outputs=output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}